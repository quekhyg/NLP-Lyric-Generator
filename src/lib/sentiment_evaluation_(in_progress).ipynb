{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18968,"status":"ok","timestamp":1654358675140,"user":{"displayName":"TeYang Lau","userId":"12955399945687383173"},"user_tz":-480},"id":"jfUfDuzEdVRn","outputId":"4fe906d3-c514-4bbc-b77b-7834de49011a"},"outputs":[],"source":["# codes to mount your google drive folder\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/Othercomputers/My Computer (1)/CS605_NLP_for_Smart_Assistants/Project/NLP-Lyric-Generator/src/bin"]},{"cell_type":"markdown","metadata":{},"source":["## How to use Sentiment class to compute sentiment scores of original and generated lyrics "]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":337,"status":"ok","timestamp":1654360928812,"user":{"displayName":"TeYang Lau","userId":"12955399945687383173"},"user_tz":-480},"id":"UFDZnvDmdadS"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\TeYan\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import sys\n","import os\n","import re\n","import numpy as np\n","\n","from gensim import downloader\n","import nltk\n","nltk.download(\"stopwords\")\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":547,"status":"ok","timestamp":1654358702770,"user":{"displayName":"TeYang Lau","userId":"12955399945687383173"},"user_tz":-480},"id":"vYo_Cwhxdk-L"},"outputs":[],"source":["### Custom Imports\n","sys.path.append('../')\n","import lib.utilities as utils\n","from sentiment import Sentiment"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1654358710935,"user":{"displayName":"TeYang Lau","userId":"12955399945687383173"},"user_tz":-480},"id":"MY_HBlwRdnQg"},"outputs":[],"source":["PATH = '../../data'"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":527,"status":"ok","timestamp":1654358724537,"user":{"displayName":"TeYang Lau","userId":"12955399945687383173"},"user_tz":-480},"id":"ZmTQjmpCdpY1","outputId":"86777fb2-8134-4c76-dfb4-680f815904e6"},"outputs":[{"data":{"text/plain":["['Because it_s Singapore.txt',\n"," 'City for the World.txt',\n"," 'Count On Me Singapore.txt',\n"," 'Everybody Is Special.txt',\n"," 'Everything I Am.txt']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dataset_file_names = os.listdir(PATH)\n","dataset_file_names[:5]"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":10779,"status":"ok","timestamp":1654358745438,"user":{"displayName":"TeYang Lau","userId":"12955399945687383173"},"user_tz":-480},"id":"ecXQbL9jdqbi"},"outputs":[],"source":["corpus = ''\n","for i,file in enumerate(dataset_file_names):\n","    text = open(PATH + '/' + file, mode='r').read()\n","    if i == 0:\n","        corpus += text\n","    else:\n","        corpus = corpus + '\\n\\n' + text"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":522,"status":"ok","timestamp":1654359018620,"user":{"displayName":"TeYang Lau","userId":"12955399945687383173"},"user_tz":-480},"id":"0mwl8wY-dvPu","outputId":"55486677-bb28-461d-f3f0-37159c7f4bed"},"outputs":[{"name":"stdout","output_type":"stream","text":["['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"]}],"source":["print(list(downloader.info()['models'].keys()))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73551,"status":"ok","timestamp":1654359182021,"user":{"displayName":"TeYang Lau","userId":"12955399945687383173"},"user_tz":-480},"id":"W_Jrnntwezmp","outputId":"13985a6c-f965-4e34-c5fc-97fa53755aa9"},"outputs":[],"source":["glove_vectors = downloader.load('glove-twitter-25')"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":358,"status":"ok","timestamp":1654360453725,"user":{"displayName":"TeYang Lau","userId":"12955399945687383173"},"user_tz":-480},"id":"2lan26nOgb1I"},"outputs":[],"source":["clean_corpus = re.sub(r'<[A-Z]+>|', '', corpus)\n","clean_corpus = re.sub(r'\\n', ' ', clean_corpus)\n","clean_corpus = re.sub('\\s+', ' ', clean_corpus)\n","clean_corpus = clean_corpus.strip()\n","\n","stop_words = set(stopwords.words(\"english\"))"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# create Sentiment object\n","sentiment = Sentiment()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['step', 'step', 'together', 'build', 'dreams', 'heart', 'heart', 'together', 'stay', 'one']\n","['love', 'singapore']\n"]}],"source":["# clean and tokenize text\n","sentiment.clean_text(\n","    original_text = clean_corpus, \n","    generated_text = 'I love Singapore', \n","    remove_stopwords=True, \n","    stop_words=stop_words\n",")\n","print(sentiment.original_tokens[:10])\n","print(sentiment.generated_tokens)\n","\n","# alternatively, if text has already been cleaned and tokenized, \n","# this step can be skipped by just initializing with the tokenized texts (uncomment next chunk)\n","# sentiment = Sentiment(\n","#     original_tokens=['step', 'step', 'together', 'build', 'dreams', 'heart', 'heart', 'together', 'stay', 'one'],\n","#     generated_tokens=['love', 'singapore']\n","# )\n","\n","# OR, if only either original or generated has been cleaned & tokenized, can just pass in one of them. But do perform\n","# clean_text() on the untokenized one afterwards\n","# sentiment = Sentiment(\n","#     original_tokens=['step', 'step', 'together', 'build', 'dreams', 'heart', 'heart', 'together', 'stay', 'one'],\n","# )\n","# sentiment.clean_text(\n","#     generated_text = 'I love Singapore', \n","#     remove_stopwords=True, \n","#     stop_words=stop_words\n","# )\n","\n","# sentiment = Sentiment(\n","#     generated_tokens=['love', 'singapore'],\n","# )\n","# sentiment.clean_text(\n","#     original_text = clean_corpus,  \n","#     remove_stopwords=True, \n","#     stop_words=stop_words\n","# )"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'original': {'neg': 0.015, 'neu': 0.613, 'pos': 0.372, 'compound': 1.0}, 'generated': {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}}\n"]}],"source":["# this step scores the vader sentiment of the original and generated text \n","sentiment.score_vader_sentiment()  \n","print(sentiment.vader_sentiment_scores)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['patriot', 'love']\n","{'patriot': array([-0.3555834 ,  0.33084852, -0.712854  , -0.77890414, -0.13798162,\n","       -0.89783   ,  0.35511184, -0.5382708 ,  0.7230024 , -0.20963497,\n","        0.224243  ,  0.0498352 , -2.29359   ,  0.602347  ,  0.24020371,\n","       -0.43554345,  0.10030401,  0.0480234 ,  0.829517  ,  0.55805457,\n","       -0.427166  ,  0.5162381 , -0.63825256, -1.70959   , -0.0421189 ],\n","      dtype=float32), 'love': array([-0.8218921 ,  0.0957543 ,  0.1357924 ,  0.24185178, -0.52851546,\n","       -0.10268509,  1.8338101 ,  0.6263027 , -0.955307  , -0.2120904 ,\n","       -0.71401703,  0.29729578, -4.3994102 , -0.22828679, -0.4999215 ,\n","       -0.0293273 ,  0.16161294, -0.7575851 , -0.20355968,  0.04964399,\n","        0.0214574 ,  0.1089946 , -0.302731  ,  0.5473858 , -0.32484618],\n","      dtype=float32)}\n"]}],"source":["# this step extracts the top n similar word vectors to each theme\n","# and get their average word vector\n","sentiment.get_theme_vector(\n","    sentiment_themes=['patriot', 'love'], \n","    embedding=glove_vectors, \n","    topn=10\n",")\n","print(sentiment.sentiment_themes)\n","print(sentiment.all_theme_vectors)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'original': {'patriot': 0.61356354, 'love': 0.9379613}, 'generated': {'patriot': 0.6044109, 'love': 0.92513263}}\n"]}],"source":["# this step scores the sentiment of the original and generated text \n","# by comparing them against the theme word vectors using cos sim \n","sentiment.score_word_vector_sentiment()  \n","print(sentiment.word_vector_sentiment_scores)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["['__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," 'all_theme_vectors',\n"," 'clean_text',\n"," 'embedding',\n"," 'generated_tokens',\n"," 'get_theme_vector',\n"," 'original_tokens',\n"," 'score_vader_sentiment',\n"," 'score_word_vector_sentiment',\n"," 'sentiment_themes',\n"," 'topn',\n"," 'vader_sentiment_scores',\n"," 'word_vector_sentiment_scores']"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# look at sentiment class methods and attributes\n","dir(sentiment)"]},{"cell_type":"markdown","metadata":{},"source":["## Below codes are for testing (can ignore)"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":352,"status":"ok","timestamp":1654361424203,"user":{"displayName":"TeYang Lau","userId":"12955399945687383173"},"user_tz":-480},"id":"UfGqtKOXkTvU","outputId":"a5cc5244-5a7f-4201-fb4f-31ee8ea6cdb8"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\TeYang\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["\n","\n","# arguments\n","# remove_stopwords = True\n","stop_words = set(stopwords.words(\"english\"))\n","\n","all_vectors = np.empty(0)\n","\n","# preprocess, tokenize and remove stopwords\n","new_text = utils.preprocess_text(clean_corpus)\n","tokens = new_text.split(\" \")\n","if remove_stopwords:\n","    tokens = [t for t in tokens if t not in stop_words]\n","\n","# get word vector of each token\n","all_vectors = [glove_vectors[t] for t in tokens if t in glove_vectors]\n","\n","all_vectors_avg = np.mean(all_vectors, axis=0)\n","\n","\n","def clean_text(text, **kwargs):\n","    \"\"\"Clean text by standardized preprocessing, tokenize, and remove stopwords if specified\n","\n","    Args:\n","      text (str): text to be cleaned\n","      remove_stopwords (bool): whether to remove stopwords or not\n","      stop_words (list): list of stopwords to be removed\n","\n","    Returns:\n","      tokens (list): tokenized cleaned text\n","    \"\"\"\n","    \n","    new_text = utils.preprocess_text(text)\n","    tokens = new_text.split(\" \")\n","    if remove_stopwords:\n","        tokens = [t for t in tokens if t not in stop_words]\n","\n","    return tokens\n"]},{"cell_type":"code","execution_count":225,"metadata":{},"outputs":[{"data":{"text/plain":["array([-2.9950729e-01,  2.5934017e-01,  1.2592184e-02, -2.1609175e-03,\n","       -3.3929065e-01,  1.2586825e-02,  1.2069796e+00, -2.1457371e-01,\n","       -1.2859058e-01, -4.3410923e-02, -3.0766147e-01,  2.8144166e-01,\n","       -3.8353212e+00,  1.9896312e-01, -7.4839495e-02,  2.0032370e-01,\n","        1.2519662e-01, -3.4012094e-01,  4.4953417e-02, -2.1795860e-01,\n","       -1.9243003e-01,  7.5987183e-02, -3.2589704e-02,  4.1797020e-02,\n","       -1.4282893e-01], dtype=float32)"]},"execution_count":225,"metadata":{},"output_type":"execute_result"}],"source":["all_vectors_avg"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"data":{"text/plain":["['step',\n"," 'step',\n"," 'together',\n"," 'build',\n"," 'dreams',\n"," 'heart',\n"," 'heart',\n"," 'together',\n"," 'stay',\n"," 'one',\n"," 'nation',\n"," 'undivided',\n"," 'back',\n"," 'back',\n"," 'together',\n"," 'brave',\n"," 'heat',\n"," 'cold',\n"," 'storms',\n"," 'hand',\n"," 'hand',\n"," 'together',\n"," 'grow',\n"," 'land',\n"," 'call',\n"," 'home',\n"," 'nothing',\n"," 'world',\n"," 'compares',\n"," 'singaporean',\n"," 'life',\n"," 'everyone',\n"," 'family',\n"," 'friend',\n"," 'neighbour',\n"," 'living',\n"," 'harmony',\n"," 'nothing',\n"," 'world',\n"," 'compares',\n"," 'island',\n"," 'home',\n"," 'love',\n"," 'know',\n"," 'never',\n"," 'alone',\n"," 'singapore',\n"," 'step',\n"," 'step',\n"," 'together',\n"," 'build',\n"," 'dreams',\n"," 'heart',\n"," 'heart',\n"," 'together',\n"," 'stay',\n"," 'one',\n"," 'nation',\n"," 'undivided',\n"," 'back',\n"," 'back',\n"," 'together',\n"," 'brave',\n"," 'heat',\n"," 'cold',\n"," 'storms',\n"," 'hand',\n"," 'hand',\n"," 'together',\n"," 'grow',\n"," 'land',\n"," 'call',\n"," 'home',\n"," 'nothing',\n"," 'world',\n"," 'compares',\n"," 'singaporean',\n"," 'life',\n"," 'everyone',\n"," 'family',\n"," 'friend',\n"," 'neighbour',\n"," 'living',\n"," 'harmony',\n"," 'nothing',\n"," 'world',\n"," 'compares',\n"," 'island',\n"," 'home',\n"," 'love',\n"," 'know',\n"," 'never',\n"," 'alone',\n"," 'nothing',\n"," 'world',\n"," 'compares',\n"," 'singaporean',\n"," 'life',\n"," 'everyone',\n"," 'family',\n"," 'friend',\n"," 'neighbour',\n"," 'living',\n"," 'harmony',\n"," 'nothing',\n"," 'world',\n"," 'compares',\n"," 'island',\n"," 'home',\n"," 'love',\n"," 'know',\n"," 'never',\n"," 'alone',\n"," 'singapore',\n"," 'singapore',\n"," 'early',\n"," 'years',\n"," 'poor',\n"," 'pioneers',\n"," 'worked',\n"," 'riverside',\n"," 'drawn',\n"," 'many',\n"," 'lands',\n"," 'dreams',\n"," 'hands',\n"," 'values',\n"," 'gave',\n"," 'us',\n"," 'survive',\n"," 'four',\n"," 'streams',\n"," 'flow',\n"," 'make',\n"," 'one',\n"," 'river',\n"," 'grow',\n"," 'came',\n"," 'together',\n"," 'one',\n"," 'live',\n"," 'side',\n"," 'side',\n"," 'share',\n"," 'island',\n"," 'wide',\n"," 'style',\n"," 'singaporean',\n"," 'home',\n"," 'home',\n"," 'grown',\n"," 'little',\n"," 'room',\n"," 'country',\n"," 'welcome',\n"," 'everyone',\n"," 'see',\n"," 'singapore',\n"," 'city',\n"," 'world',\n"," 'reach',\n"," 'hands',\n"," 'friends',\n"," 'far',\n"," 'lands',\n"," 'bridge',\n"," 'dividing',\n"," 'streams',\n"," 'together',\n"," 'build',\n"," 'world',\n"," 'filled',\n"," 'hope',\n"," 'singapore',\n"," 'dream',\n"," 'home',\n"," 'home',\n"," 'grown',\n"," 'little',\n"," 'room',\n"," 'country',\n"," 'welcome',\n"," 'everyone',\n"," 'see',\n"," 'singapore',\n"," 'city',\n"," 'world',\n"," 'home',\n"," 'home',\n"," 'grown',\n"," 'little',\n"," 'room',\n"," 'country',\n"," 'welcome',\n"," 'everyone',\n"," 'see',\n"," 'singapore',\n"," 'city',\n"," 'world',\n"," 'home',\n"," 'home',\n"," 'grown',\n"," 'little',\n"," 'room',\n"," 'country',\n"," 'welcome',\n"," 'everyone',\n"," 'see',\n"," 'singapore',\n"," 'city',\n"," 'world',\n"," 'nation',\n"," 'home',\n"," 'safe',\n"," 'free',\n"," 'city',\n"," 'world',\n"," 'vision',\n"," 'tomorrow',\n"," 'believe',\n"," 'believe',\n"," 'goal',\n"," 'singapore',\n"," 'achieve',\n"," 'achieve',\n"," 'part',\n"," 'stand',\n"," 'together',\n"," 'heart',\n"," 'heart',\n"," 'going',\n"," 'show',\n"," 'world',\n"," 'singapore',\n"," 'achieve',\n"," 'achieve',\n"," 'something',\n"," 'road',\n"," 'strive',\n"," 'told',\n"," 'dream',\n"," 'bold',\n"," 'try',\n"," 'spirit',\n"," 'air',\n"," 'feeling',\n"," 'share',\n"," 'going',\n"," 'build',\n"," 'better',\n"," 'life',\n"," 'achieve',\n"," 'achieve',\n"," 'count',\n"," 'singapore',\n"," 'count',\n"," 'give',\n"," 'best',\n"," 'part',\n"," 'stand',\n"," 'together',\n"," 'heart',\n"," 'heart',\n"," 'going',\n"," 'show',\n"," 'world',\n"," 'singapore',\n"," 'achieve',\n"," 'achieve',\n"," 'count',\n"," 'singapore',\n"," 'count',\n"," 'singapore',\n"," 'count',\n"," 'give',\n"," 'best',\n"," 'together',\n"," 'singapore',\n"," 'singapore',\n"," 'achieve',\n"," 'together',\n"," 'singapore',\n"," 'achieve',\n"," 'together',\n"," 'singapore',\n"," 'achieve',\n"," 'singapore',\n"," 'best',\n"," 'together',\n"," 'singapore',\n"," 'singapore',\n"," 'together',\n"," 'singapore',\n"," 'singapore',\n"," 'together',\n"," 'singapore',\n"," 'singapore',\n"," 'share',\n"," 'thought',\n"," 'everyone',\n"," 'escape',\n"," 'ordinary',\n"," 'world',\n"," 'reach',\n"," 'hand',\n"," 'others',\n"," 'dreams',\n"," 'live',\n"," 'unfurled',\n"," 'everybody',\n"," 'special',\n"," 'hope',\n"," 'see',\n"," 'hidden',\n"," 'dimensions',\n"," 'set',\n"," 'free',\n"," 'everybody',\n"," 'special',\n"," 'spark',\n"," 'everyone',\n"," 'bursts',\n"," 'life',\n"," 'share',\n"," 'thought',\n"," 'everyone',\n"," 'blessing',\n"," 'life',\n"," 'take',\n"," 'courage',\n"," 'hold',\n"," 'break',\n"," 'stop',\n"," 'asking',\n"," 'everybody',\n"," 'special',\n"," 'hope',\n"," 'see',\n"," 'hidden',\n"," 'dimensions',\n"," 'set',\n"," 'free',\n"," 'everybody',\n"," 'special',\n"," 'spark',\n"," 'everyone',\n"," 'bursts',\n"," 'life',\n"," 'touch',\n"," 'heart',\n"," 'feel',\n"," 'spirit',\n"," 'soul',\n"," 'feel',\n"," 'love',\n"," 'inside',\n"," 'heart',\n"," 'touch',\n"," 'heart',\n"," 'feel',\n"," 'spirit',\n"," 'make',\n"," 'whole',\n"," 'everybody',\n"," 'special',\n"," 'hope',\n"," 'see',\n"," 'hidden',\n"," 'dimensions',\n"," 'set',\n"," 'free',\n"," 'everybody',\n"," 'special',\n"," 'spark',\n"," 'everyone',\n"," 'bursts',\n"," 'life',\n"," 'want',\n"," 'understand',\n"," 'everything',\n"," 'show',\n"," 'strong',\n"," 'teach',\n"," 'weak',\n"," 'find',\n"," 'anything',\n"," 'seek',\n"," 'colours',\n"," 'tapestry',\n"," 'colours',\n"," 'never',\n"," 'seen',\n"," 'home',\n"," 'homeland',\n"," 'every',\n"," 'shining',\n"," 'crystal',\n"," 'sand',\n"," 'everything',\n"," 'homeland',\n"," 'hold',\n"," 'precious',\n"," 'jewel',\n"," 'hand',\n"," 'everything',\n"," 'teacher',\n"," 'teach',\n"," 'kind',\n"," 'quick',\n"," 'embrace',\n"," 'slow',\n"," 'close',\n"," 'mind',\n"," 'reach',\n"," 'star',\n"," 'universe',\n"," 'time',\n"," 'fabric',\n"," 'destiny',\n"," 'home',\n"," 'homeland',\n"," 'sea',\n"," 'shining',\n"," 'crystals',\n"," 'sand',\n"," 'everything',\n"," 'singapore',\n"," 'people',\n"," 'could',\n"," 'want',\n"," 'nothing',\n"," 'singapore',\n"," 'hold',\n"," 'precious',\n"," 'jewel',\n"," 'hands',\n"," 'everything',\n"," 'new',\n"," 'moon',\n"," 'arising',\n"," 'stormy',\n"," 'sea',\n"," 'youthful',\n"," 'bright',\n"," 'bearing',\n"," 'hope',\n"," 'tranquil',\n"," 'reach',\n"," 'moon',\n"," 'savour',\n"," 'freedom',\n"," 'truth',\n"," 'love',\n"," 'new',\n"," 'moon',\n"," 'arising',\n"," 'stormy',\n"," 'sea',\n"," 'five',\n"," 'stars',\n"," 'arising',\n"," 'stormy',\n"," 'sea',\n"," 'lamp',\n"," 'guide',\n"," 'way',\n"," 'lamp',\n"," 'see',\n"," 'reach',\n"," 'stars',\n"," 'savour',\n"," 'freedom',\n"," 'truth',\n"," 'love',\n"," 'five',\n"," 'stars',\n"," 'arising',\n"," 'stormy',\n"," 'sea',\n"," 'new',\n"," 'flag',\n"," 'arising',\n"," 'stormy',\n"," 'sea',\n"," 'crimson',\n"," 'blood',\n"," 'mankind',\n"," 'yet',\n"," 'white',\n"," 'pure',\n"," 'free',\n"," 'reach',\n"," 'flag',\n"," 'savour',\n"," 'freedom',\n"," 'truth',\n"," 'love',\n"," 'new',\n"," 'flag',\n"," 'arising',\n"," 'happy',\n"," 'proud',\n"," 'new',\n"," 'flag',\n"," 'arising',\n"," 'stormy',\n"," 'sea',\n"," 'crimson',\n"," 'blood',\n"," 'mankind',\n"," 'yet',\n"," 'white',\n"," 'pure',\n"," 'free',\n"," 'reach',\n"," 'flag',\n"," 'savour',\n"," 'freedom',\n"," 'truth',\n"," 'love',\n"," 'new',\n"," 'flag',\n"," 'arising',\n"," 'happy',\n"," 'proud',\n"," 'place',\n"," 'amazed',\n"," 'children',\n"," 'play',\n"," 'best',\n"," 'friends',\n"," 'stay',\n"," 'different',\n"," 'tastes',\n"," 'live',\n"," 'different',\n"," 'ways',\n"," 'smile',\n"," 'face',\n"," 'bring',\n"," 'us',\n"," 'closer',\n"," 'every',\n"," 'day',\n"," 'place',\n"," 'common',\n"," 'space',\n"," 'share',\n"," 'dreams',\n"," 'one',\n"," 'goal',\n"," 'chase',\n"," 'come',\n"," 'join',\n"," 'pace',\n"," 'win',\n"," 'race',\n"," 'together',\n"," 'make',\n"," 'miracles',\n"," 'come',\n"," 'way',\n"," 'see',\n"," 'familiar',\n"," 'faces',\n"," 'neighbourhood',\n"," 'share',\n"," 'stories',\n"," 'dating',\n"," 'back',\n"," 'childhood',\n"," 'find',\n"," 'hand',\n"," 'lift',\n"," 'fall',\n"," 'sure',\n"," 'safe',\n"," 'lost',\n"," 'stand',\n"," 'one',\n"," 'build',\n"," 'home',\n"," 'join',\n"," 'hands',\n"," 'grow',\n"," 'fruits',\n"," 'harmony',\n"," 'put',\n"," 'hearts',\n"," 'together',\n"," 'stand',\n"," 'test',\n"," 'belong',\n"," 'give',\n"," 'best',\n"," 'place',\n"," 'common',\n"," 'space',\n"," 'share',\n"," 'dreams',\n"," 'one',\n"," 'goal',\n"," 'chase',\n"," 'come',\n"," 'join',\n"," 'pace',\n"," 'win',\n"," 'race',\n"," 'belong',\n"," 'know',\n"," 'love',\n"," 'see',\n"," 'familiar',\n"," 'faces',\n"," 'neighbourhood',\n"," 'share',\n"," 'stories',\n"," 'dating',\n"," 'back',\n"," 'childhood',\n"," 'find',\n"," 'hand',\n"," 'lift',\n"," 'fall',\n"," 'sure',\n"," 'safe',\n"," 'lost',\n"," 'stand',\n"," 'one',\n"," 'build',\n"," 'home',\n"," 'join',\n"," 'hands',\n"," 'grow',\n"," 'fruits',\n"," 'harmony',\n"," 'put',\n"," 'hearts',\n"," 'together',\n"," 'stand',\n"," 'test',\n"," 'belong',\n"," 'give',\n"," 'best',\n"," 'place',\n"," 'common',\n"," 'space',\n"," 'share',\n"," 'dreams',\n"," 'one',\n"," 'goal',\n"," 'chase',\n"," 'come',\n"," 'join',\n"," 'pace',\n"," 'win',\n"," 'race',\n"," 'belong',\n"," 'know',\n"," 'love',\n"," 'belong',\n"," 'close',\n"," 'one',\n"," 'big',\n"," 'family',\n"," 'belong',\n"," 'heart',\n"," 'always',\n"," 'whenever',\n"," 'feeling',\n"," 'low',\n"," 'look',\n"," 'around',\n"," 'know',\n"," 'place',\n"," 'stay',\n"," 'within',\n"," 'wherever',\n"," 'may',\n"," 'choose',\n"," 'go',\n"," 'always',\n"," 'recall',\n"," 'city',\n"," 'know',\n"," 'every',\n"," 'street',\n"," 'shore',\n"," 'sail',\n"," 'river',\n"," 'brings',\n"," 'us',\n"," 'life',\n"," 'winding',\n"," 'singapore',\n"," 'home',\n"," 'truly',\n"," 'know',\n"," 'must',\n"," 'dreams',\n"," 'wait',\n"," 'river',\n"," 'always',\n"," 'flows',\n"," 'home',\n"," 'surely',\n"," 'senses',\n"," 'tell',\n"," 'alone',\n"," 'know',\n"," 'home',\n"," 'troubles',\n"," 'go',\n"," 'find',\n"," 'way',\n"," 'start',\n"," 'anew',\n"," 'comfort',\n"," 'knowledge',\n"," 'home',\n"," 'people',\n"," 'build',\n"," 'dreams',\n"," 'together',\n"," 'like',\n"," 'done',\n"," 'like',\n"," 'river',\n"," 'brings',\n"," 'us',\n"," 'life',\n"," 'always',\n"," 'singapore',\n"," 'home',\n"," 'truly',\n"," 'know',\n"," 'must',\n"," 'dreams',\n"," 'wait',\n"," 'river',\n"," 'always',\n"," 'flows',\n"," 'home',\n"," 'surely',\n"," 'senses',\n"," 'tell',\n"," 'alone',\n"," 'know',\n"," 'home',\n"," 'home',\n"," 'truly',\n"," 'know',\n"," 'must',\n"," 'dreams',\n"," 'wait',\n"," 'river',\n"," 'always',\n"," 'flows',\n"," 'home',\n"," 'surely',\n"," 'senses',\n"," 'tell',\n"," 'alone',\n"," 'know',\n"," 'home',\n"," 'know',\n"," 'home',\n"," 'know',\n"," 'home',\n"," 'quarter',\n"," 'century',\n"," 'worked',\n"," 'hard',\n"," 'free',\n"," 'heartaches',\n"," 'pains',\n"," 'stood',\n"," 'many',\n"," 'many',\n"," 'changes',\n"," 'hearts',\n"," 'become',\n"," 'like',\n"," 'family',\n"," 'see',\n"," 'yeah',\n"," 'see',\n"," 'ooh',\n"," 'hands',\n"," 'hearts',\n"," 'given',\n"," 'take',\n"," 'time',\n"," 'understand',\n"," 'things',\n"," 'slowly',\n"," 'become',\n"," 'know',\n"," 'really',\n"," 'really',\n"," 'smile',\n"," 'say',\n"," 'say',\n"," 'hey',\n"," 'love',\n"," 'singapore',\n"," 'love',\n"," 'singapore',\n"," 'love',\n"," 'singapore',\n"," 'singapore',\n"," 'love',\n"," 'love',\n"," 'much',\n"," 'people',\n"," 'tried',\n"," 'beyond',\n"," 'written',\n"," 'faces',\n"," 'tell',\n"," 'heartaches',\n"," 'pains',\n"," 'stood',\n"," 'many',\n"," 'many',\n"," 'changes',\n"," 'hearts',\n"," 'become',\n"," 'like',\n"," 'family',\n"," 'see',\n"," 'yeah',\n"," 'see',\n"," 'ooh',\n"," 'love',\n"," 'singapore',\n"," 'love',\n"," 'singapore',\n"," 'love',\n"," 'singapore',\n"," 'singapore',\n"," 'love',\n"," 'love',\n"," 'much',\n"," 'dream',\n"," 'starting',\n"," 'life',\n"," 'hope',\n"," 'flame',\n"," 'alight',\n"," 'moment',\n"," 'freedom',\n"," 'calling',\n"," 'start',\n"," 'new',\n"," 'beginning',\n"," 'heart',\n"," 'beats',\n"," 'step',\n"," 'time',\n"," 'spirits',\n"," 'renew',\n"," 'living',\n"," 'together',\n"," 'sun',\n"," 'hearts',\n"," 'beat',\n"," 'one',\n"," 'one',\n"," 'heartbeat',\n"," 'heartbeat',\n"," 'one',\n"," 'else',\n"," 'heartbeat',\n"," 'always',\n"," 'together',\n"," 'united',\n"," 'heartbeat',\n"," 'time',\n"," 'passed',\n"," 'us',\n"," 'heartbeat',\n"," 'reach',\n"," 'sky',\n"," 'heartbeat',\n"," 'always',\n"," 'people',\n"," 'country',\n"," 'family',\n"," 'nothing',\n"," 'stand',\n"," 'way',\n"," 'strong',\n"," 'today',\n"," 'flame',\n"," 'alive',\n"," 'life',\n"," 'heartbeat',\n"," 'heartbeat',\n"," 'one',\n"," 'else',\n"," 'heartbeat',\n"," 'always',\n"," 'together',\n"," 'united',\n"," 'heartbeat',\n"," 'time',\n"," 'passed',\n"," 'us',\n"," 'heartbeat',\n"," 'reach',\n"," 'sky',\n"," 'always',\n"," 'people',\n"," 'country',\n"," 'family',\n"," 'years',\n"," 'grown',\n"," 'part',\n"," 'cared',\n"," 'opened',\n"," 'way',\n"," 'happy',\n"," 'beautiful',\n"," 'life',\n"," 'make',\n"," 'feel',\n"," 'warm',\n"," 'safe',\n"," 'give',\n"," 'hope',\n"," 'brighter',\n"," 'days',\n"," 'little',\n"," 'things',\n"," 'share',\n"," 'love',\n"," 'joy',\n"," 'air',\n"," 'children',\n"," 'laughter',\n"," 'everywhere',\n"," 'favourite',\n"," 'things',\n"," 'little',\n"," 'things',\n"," 'share',\n"," 'love',\n"," 'joy',\n"," 'air',\n"," 'children',\n"," 'laughter',\n"," 'everywhere',\n"," 'favourite',\n"," 'things',\n"," 'years',\n"," 'grown',\n"," 'accustomed',\n"," 'ways',\n"," 'matter',\n"," 'warms',\n"," 'heart',\n"," 'know',\n"," 'always',\n"," 'little',\n"," 'things',\n"," 'share',\n"," 'love',\n"," 'joy',\n"," 'air',\n"," 'children',\n"," 'laughter',\n"," 'everywhere',\n"," 'favourite',\n"," 'things',\n"," 'little',\n"," 'things',\n"," 'share',\n"," 'love',\n"," 'joy',\n"," 'air',\n"," 'children',\n"," 'laughter',\n"," 'everywhere',\n"," 'favourite',\n"," 'things',\n"," 'years',\n"," 'learnt',\n"," 'share',\n"," 'destiny',\n"," 'matter',\n"," 'good',\n"," 'others',\n"," 'may',\n"," 'one',\n"," 'cares',\n"," 'like',\n"," 'care',\n"," 'cause',\n"," 'deep',\n"," ...]"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["clean_text(clean_corpus, remove_stopwords=True, stop_words=stop_words)"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1654361429918,"user":{"displayName":"TeYang Lau","userId":"12955399945687383173"},"user_tz":-480},"id":"KoIn69rSg2oX","outputId":"b41c21e8-05e5-45b1-9e8e-54e83ea3e259"},"outputs":[],"source":["# arguments\n","sentiment_themes = [\"patriot\", \"love\"]\n","all_theme_vector = {}\n","\n","for theme in sentiment_themes:\n","    most_sim = [x[0] for x in glove_vectors.most_similar(theme, topn=10)]\n","    theme_vector = np.mean(glove_vectors[most_sim], axis=0)\n","    all_theme_vector[theme] = theme_vector\n","\n","\n","def get_theme_vector(sentiment_themes, embedding):\n","    \"\"\"Compute the average vector for each given theme based on a specified word embedding\n","\n","    Args:\n","      sentiment_themes (list): list of strings of the sentiment themes to compute\n","      embedding (gensim Word2VecKeyedVectors): the word embedding for extracting theme vectors\n","\n","    Returns:\n","      all_theme_vector (dict): contains the theme & average word vector pairs\n","    \"\"\"\n","\n","    # initialize\n","    all_theme_vector = {}\n","\n","    for theme in sentiment_themes:\n","        most_sim = [x[0] for x in embedding.most_similar(theme, topn=10)]\n","        theme_vector = np.mean(embedding[most_sim], axis=0)\n","        all_theme_vector[theme] = theme_vector\n","\n","    return all_theme_vector\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/plain":["{'patriot': array([-0.3555834 ,  0.33084852, -0.712854  , -0.77890414, -0.13798162,\n","        -0.89783   ,  0.35511184, -0.5382708 ,  0.7230024 , -0.20963497,\n","         0.224243  ,  0.0498352 , -2.29359   ,  0.602347  ,  0.24020371,\n","        -0.43554345,  0.10030401,  0.0480234 ,  0.829517  ,  0.55805457,\n","        -0.427166  ,  0.5162381 , -0.63825256, -1.70959   , -0.0421189 ],\n","       dtype=float32),\n"," 'love': array([-0.8218921 ,  0.0957543 ,  0.1357924 ,  0.24185178, -0.52851546,\n","        -0.10268509,  1.8338101 ,  0.6263027 , -0.955307  , -0.2120904 ,\n","        -0.71401703,  0.29729578, -4.3994102 , -0.22828679, -0.4999215 ,\n","        -0.0293273 ,  0.16161294, -0.7575851 , -0.20355968,  0.04964399,\n","         0.0214574 ,  0.1089946 , -0.302731  ,  0.5473858 , -0.32484618],\n","       dtype=float32)}"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["all_theme_vector"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/plain":["(25,)"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(glove_vectors[['you', 'haha']], axis=0).shape"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["[('there', 0.9834133386611938),\n"," ('way', 0.9821949601173401),\n"," ('it', 0.9725627899169922),\n"," ('every', 0.9713938236236572),\n"," ('all', 0.9710874557495117),\n"," ('have', 0.9680160284042358),\n"," ('this', 0.9659813642501831),\n"," ('where', 0.9656959772109985),\n"," ('only', 0.9645785093307495),\n"," ('and', 0.9639434218406677)]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["glove_vectors.similar_by_vector(all_vectors_avg)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to\n","[nltk_data]     C:\\Users\\TeYan\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["{'neg': 0.0, 'neu': 0.519, 'pos': 0.481, 'compound': 0.5719}"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["sid = SentimentIntensityAnalyzer()\n","sid.polarity_scores('hi how are you happy')"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["{'neg': 0.0, 'neu': 0.213, 'pos': 0.787, 'compound': 0.5719}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["sid.polarity_scores('hi happy')"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["class Sentiment:\n","    \"\"\"\n","    A class for comparing sentiment scores of original text and generated text\n","\n","    ...\n","    Attributes\n","    ----------\n","    original_tokens : list\n","        list of tokens for original text (defaults to None)\n","    generated_tokens : list\n","        list of tokens for generated text (defaults to None)\n","    ----------------------------------------------------------------------\n","    \"\"\"\n","\n","    def __init__(self, original_tokens=None, generated_tokens=None):\n","\n","        self.original_tokens = original_tokens\n","        self.generated_tokens = generated_tokens\n","\n","    def clean_text(self, **kwargs):\n","        \"\"\"Clean text by standardized preprocessing, tokenize, and remove stopwords if specified\n","\n","        Args:\n","        original_text (str, optional): original text to be cleaned\n","        generated_text (str, optional): generated text to be cleaned\n","        remove_stopwords (bool): whether to remove stopwords or not\n","        stop_words (list): list of stopwords to be removed. Required if remove_stopwords is True\n","        \"\"\"\n","        \n","        if kwargs.get('original_text'):\n","            original_new_text = utils.preprocess_text(kwargs['original_text'])\n","            original_tokens = original_new_text.split(\" \")\n","            if kwargs.get('remove_stopwords'):\n","                original_tokens = [t for t in original_tokens if t not in kwargs['stop_words']]\n","            self.original_tokens = original_tokens\n","\n","        if kwargs.get('generated_text'):\n","            generated_new_text = utils.preprocess_text(kwargs['generated_text'])\n","            generated_tokens = generated_new_text.split(\" \")\n","            if kwargs.get('remove_stopwords'):\n","                generated_tokens = [t for t in generated_tokens if t not in kwargs['stop_words']]\n","            self.generated_tokens = generated_tokens\n","\n","\n","    @staticmethod\n","    def get_theme_vector(sentiment_themes, embedding, topn=10):\n","        \"\"\"Compute the average vector for each given theme based on a specified word embedding\n","\n","        Args:\n","        sentiment_themes (list): list of strings of the sentiment themes to compute\n","        embedding (gensim Word2VecKeyedVectors): the word embedding for extracting theme vectors\n","        topn (int): determine the top n similar words to the theme for extraction (defaults to 10)\n","        \"\"\"\n","\n","        # initialize\n","        all_theme_vectors = {}\n","\n","        sentiment_themes = [theme.lower() for theme in sentiment_themes]\n","        for theme in sentiment_themes:\n","            most_sim = [x[0] for x in embedding.most_similar(theme, topn=topn)]\n","            theme_vector = np.mean(embedding[most_sim], axis=0)\n","            all_theme_vectors[theme] = theme_vector\n","\n","        Sentiment.sentiment_themes = sentiment_themes\n","        Sentiment.all_theme_vectors = all_theme_vectors\n","        Sentiment.embedding = embedding\n","        Sentiment.topn = topn\n","\n","\n","    def score_word_vector_sentiment(self):\n","        \"\"\"Compute the cosine similarity score of each text with each theme\n","        \"\"\"\n","        \n","        # check if original & generated tokens exist\n","        if not self.original_tokens or not self.generated_tokens:\n","            error_text = \"\"\"ORIGINAL or GENERATED tokens does NOT exist.\n","            Either pass it into the class using Sentimentality(original_text=tokens, generated_text=tokens)\n","            or use the clean_text method. \"\"\"\n","            raise AttributeError(error_text)\n","\n","        # check if theme average vectors exist\n","        try: self.all_theme_vectors\n","        except AttributeError as error:\n","            # print(error)\n","\n","            error_text = \"\"\"No theme vector exists. Run get_theme_vector() to compute them.\n","            For more info, refer to help(Sentimentality.get_theme_vector)\"\"\"\n","            raise AttributeError(error_text)\n","\n","        # initialize\n","        sentiment_scores = {'original':{}, 'generated':{}}\n","\n","        # get mean word vector of original and generated text\n","        original_vectors = [self.embedding[t] for t in self.original_tokens if t in self.embedding]\n","        original_vectors_avg = np.mean(original_vectors, axis=0)\n","        generated_vectors = [self.embedding[t] for t in self.generated_tokens if t in self.embedding]\n","        generated_vectors_avg = np.mean(generated_vectors, axis=0)\n","\n","        for theme, vector in self.all_theme_vectors.items():\n","            original_cossim = np.dot(original_vectors_avg,vector)/(norm(original_vectors_avg)*norm(vector))\n","            sentiment_scores['original'][theme] = original_cossim\n","            generated_cossim = np.dot(generated_vectors_avg,vector)/(norm(generated_vectors_avg)*norm(vector))\n","            sentiment_scores['generated'][theme] = generated_cossim\n","\n","        self.word_vector_sentiment_scores = sentiment_scores\n","\n","    def score_vader_sentiment(self):\n","        \"\"\"Compute the vader sentiment score of original & generated text \n","        \"\"\"\n","        \n","        # check if original & generated tokens exist\n","        if not self.original_tokens or not self.generated_tokens:\n","            error_text = \"\"\"ORIGINAL or GENERATED tokens does NOT exist.\n","            Either pass it into the class using Sentimentality(original_text=tokens, generated_text=tokens)\n","            or use the clean_text method. \"\"\"\n","            raise AttributeError(error_text)\n","\n","        # initialize\n","        sentiment_scores = {'original':{}, 'generated':{}}\n","\n","        # get mean vader sentiment score for original & generated text\n","        sid = SentimentIntensityAnalyzer()\n","\n","        original_scores = sid.polarity_scores(' '.join(self.original_tokens))\n","        generated_scores = sid.polarity_scores(' '.join(self.generated_tokens))\n","        sentiment_scores['original'] = original_scores\n","        sentiment_scores['generated'] = generated_scores\n","\n","        self.vader_sentiment_scores = sentiment_scores"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["stop_words = set(stopwords.words(\"english\"))\n","from numpy.linalg import norm"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["sentiment = Sentiment()"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["sentiment.clean_text(\n","    original_text = clean_corpus, \n","    generated_text = 'I love Singapore', \n","    remove_stopwords=True, \n","    stop_words=stop_words\n",")\n","sentiment.get_theme_vector(['patriot', 'love'], glove_vectors)\n","sentiment.score_word_vector_sentiment()\n","sentiment.score_vader_sentiment()"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/plain":["{'original': {'neg': 0.015, 'neu': 0.613, 'pos': 0.372, 'compound': 1.0},\n"," 'generated': {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}}"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["sentiment.vader_sentiment_scores"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["['__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," 'all_theme_vectors',\n"," 'clean_text',\n"," 'embedding',\n"," 'generated_tokens',\n"," 'get_theme_vector',\n"," 'original_tokens',\n"," 'score_vader_sentiment',\n"," 'score_word_vector_sentiment',\n"," 'sentiment_themes',\n"," 'topn',\n"," 'vader_sentiment_scores',\n"," 'word_vector_sentiment_scores']"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["dir(sentiment)"]},{"cell_type":"code","execution_count":245,"metadata":{},"outputs":[{"data":{"text/plain":["{'original': {'patriot': 0.6135636, 'love': 0.9379613},\n"," 'generated': {'patriot': 0.6044109, 'love': 0.92513263}}"]},"execution_count":245,"metadata":{},"output_type":"execute_result"}],"source":["sent.sentiment_scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMxPtOVM7DH04iDJndK2CnH","collapsed_sections":[],"name":"sentiment_evaluation.ipynb","provenance":[]},"interpreter":{"hash":"68342d1c4a48094469edef9f9554beb5e1a3b5fe73fb389c236252c03f11d3ac"},"kernelspec":{"display_name":"Python 3.8.8 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
