{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1sqwH29gCCae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2915,
     "status": "ok",
     "timestamp": 1655027049035,
     "user": {
      "displayName": "QUEK HAO YONG, GABRIEL _",
      "userId": "08861584446371432378"
     },
     "user_tz": -480
    },
    "id": "1sqwH29gCCae",
    "outputId": "5fd4e523-ff43-49de-c7dd-ca08a1022849"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/SMU_MITB_NLP/Group Project/NLP-Lyric-Generator/src/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed94d9c",
   "metadata": {
    "id": "eed94d9c"
   },
   "outputs": [],
   "source": [
    "### Standard Imports\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18f77d0b",
   "metadata": {
    "id": "18f77d0b"
   },
   "outputs": [],
   "source": [
    "### Custom Imports\n",
    "sys.path.append('../')\n",
    "import lib.utilities as utils\n",
    "import lib.autoencoder_utilities as ae_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "QsvWeCUNxSvd",
   "metadata": {
    "id": "QsvWeCUNxSvd"
   },
   "outputs": [],
   "source": [
    "### Text Parameters\n",
    "start_token = '<cls>'\n",
    "end_token = '<eos>'\n",
    "pad_token = '<pad>'\n",
    "unk_token = '<unk>'\n",
    "newline_token = '<new>'\n",
    "\n",
    "### General Parameters\n",
    "random_seed = 2022\n",
    "model_folder = '../../../autoencoder/gru/v1'\n",
    "model_name = 'ae_gru'\n",
    "\n",
    "### Model Parameters\n",
    "val_split = 0.2\n",
    "window_len = 15\n",
    "batch_size = 64\n",
    "enc_dim, dec_dim = 256, 256\n",
    "learn_rate = 0.001\n",
    "epochs = 50\n",
    "dropout = 0.05\n",
    "recurrent_dropout = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016082cc",
   "metadata": {
    "id": "016082cc"
   },
   "outputs": [],
   "source": [
    "os.makedirs(model_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f082fd",
   "metadata": {
    "id": "e1f082fd"
   },
   "outputs": [],
   "source": [
    "### Load Data\n",
    "corpus = utils.load_corpus()\n",
    "train_corpus, val_corpus, train_files, val_files = utils.split_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f28578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There_s A Part For Everyone.txt',\n",
       " 'Voices From The Heart.txt',\n",
       " 'We Are The Ones.txt',\n",
       " 'Together.txt',\n",
       " 'Because it_s Singapore.txt',\n",
       " 'Sing A Song Of Singapore.txt',\n",
       " 'Shine for Singapore.txt',\n",
       " 'Five Stars Arising.txt',\n",
       " 'Count On Me Singapore.txt',\n",
       " 'Will You.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52adf0e5",
   "metadata": {
    "id": "52adf0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Pre-Processing Text\n",
    "_, word_count, index_to_vocab, vocab_to_index, _, _ = utils.tokenize_corpus(corpus,\n",
    "                                                                            window_length = window_len,\n",
    "                                                                            end_token = end_token,\n",
    "                                                                            start_token = start_token,\n",
    "                                                                            pad_token = pad_token,\n",
    "                                                                            unk_token = unk_token,\n",
    "                                                                            newline_token = newline_token)\n",
    "vocab_size = len(word_count)\n",
    "\n",
    "train_words, _, _, _, train_songs, train_songs_token_ind = utils.tokenize_corpus(train_corpus,\n",
    "                                                                       window_length = window_len,\n",
    "                                                                       index_to_vocab = index_to_vocab,\n",
    "                                                                       vocab_to_index = vocab_to_index,\n",
    "                                                                       end_token = end_token,\n",
    "                                                                       start_token = start_token,\n",
    "                                                                       pad_token = pad_token,\n",
    "                                                                       unk_token = unk_token,\n",
    "                                                                       newline_token = newline_token)\n",
    "\n",
    "val_words, _, _, _, _, val_songs_token_ind = utils.tokenize_corpus(val_corpus,\n",
    "                                                           window_length = window_len,\n",
    "                                                           index_to_vocab = index_to_vocab,\n",
    "                                                           vocab_to_index = vocab_to_index,\n",
    "                                                           end_token = end_token,\n",
    "                                                           start_token = start_token,\n",
    "                                                           pad_token = pad_token,\n",
    "                                                           unk_token = unk_token,\n",
    "                                                           newline_token = newline_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "FwuVvlRF-cgL",
   "metadata": {
    "id": "FwuVvlRF-cgL"
   },
   "outputs": [],
   "source": [
    "train_x_encoder, train_x_decoder, train_y = ae_utils.construct_seq_data(train_songs_token_ind, window_len)\n",
    "val_x_encoder, val_x_decoder, val_y = ae_utils.construct_seq_data(val_songs_token_ind, window_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "KY-CHZ0CcP2C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1655027055301,
     "user": {
      "displayName": "QUEK HAO YONG, GABRIEL _",
      "userId": "08861584446371432378"
     },
     "user_tz": -480
    },
    "id": "KY-CHZ0CcP2C",
    "outputId": "051a6bed-9f09-43a2-b40a-c6d5252a6c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'there', 'is', 'still', 'a', 'long', 'long', 'way', 'to', 'go', '<new>', 'with', 'all', 'of', 'my']\n",
      "['<new>', 'and', 'there', 'is', 'still', 'a', 'long', 'long', 'way', 'to', 'go', '<new>', 'with', 'all', 'of']\n",
      "my\n",
      "['and', 'beyond', '<new>', 'as', 'one', 'we', 'will', 'stand', 'we', 'are', 'singapore', '<new>', 'it', 'is', 'here']\n",
      "['far', 'and', 'beyond', '<new>', 'as', 'one', 'we', 'will', 'stand', 'we', 'are', 'singapore', '<new>', 'it', 'is']\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "rand_int = np.random.randint(0, len(train_x_encoder), 1)[0]\n",
    "print([index_to_vocab.get(x) for x in train_x_encoder[rand_int]])\n",
    "print([index_to_vocab.get(x) for x in train_x_decoder[rand_int]])\n",
    "print(index_to_vocab.get(train_y[rand_int]))\n",
    "\n",
    "rand_int = np.random.randint(0, len(val_x_encoder), 1)[0]\n",
    "print([index_to_vocab.get(x) for x in val_x_encoder[rand_int]])\n",
    "print([index_to_vocab.get(x) for x in val_x_decoder[rand_int]])\n",
    "print(index_to_vocab.get(val_y[rand_int]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Dk3I2VI1Nza3",
   "metadata": {
    "id": "Dk3I2VI1Nza3"
   },
   "outputs": [],
   "source": [
    "train_dataset = ae_utils.construct_datasets(train_x_encoder, train_x_decoder, train_y,\n",
    "                                            random_seed = random_seed,\n",
    "                                            batch_size = batch_size,\n",
    "                                            vocab_size = vocab_size)\n",
    "val_dataset = ae_utils.construct_datasets(val_x_encoder, val_x_decoder, val_y,\n",
    "                                            random_seed = random_seed,\n",
    "                                            batch_size = batch_size,\n",
    "                                            vocab_size = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "202bf8be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1655027057350,
     "user": {
      "displayName": "QUEK HAO YONG, GABRIEL _",
      "userId": "08861584446371432378"
     },
     "user_tz": -480
    },
    "id": "202bf8be",
    "outputId": "674d37c5-b634-4a4e-e86b-4f4695a7f5be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer encoder_gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer decoder_gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"ae_gru\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder_gru (GRU)              [(None, 256),        998400      ['encoder_input[0][0]']          \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_gru (GRU)              (None, 256)          998400      ['decoder_input[0][0]',          \n",
      "                                                                  'encoder_gru[0][1]']            \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1042)         267794      ['decoder_gru[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,264,594\n",
      "Trainable params: 2,264,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "encoder_input = layers.Input(shape=(window_len,vocab_size), name = 'encoder_input')\n",
    "\n",
    "# Return state in addition to output\n",
    "encoder_output, encoder_hidden_state = layers.GRU(enc_dim,\n",
    "                                                  dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
    "                                                  return_state=True, name = \"encoder_gru\")(encoder_input)\n",
    "\n",
    "# Decoder\n",
    "decoder_input = layers.Input(shape=(window_len,vocab_size), name = 'decoder_input')\n",
    "\n",
    "# Pass the encoder state to a new LSTM, as initial state\n",
    "decoder_output = layers.GRU(dec_dim,\n",
    "                            dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
    "                            name=\"decoder_gru\")(decoder_input, initial_state=[encoder_hidden_state])\n",
    "output = layers.Dense(vocab_size, name = 'output', activation = 'softmax')(decoder_output)\n",
    "\n",
    "model = tf.keras.Model((encoder_input, decoder_input), output, name = model_name)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83bb01d4",
   "metadata": {
    "id": "83bb01d4"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "MS_vUBeRM5EK",
   "metadata": {
    "id": "MS_vUBeRM5EK"
   },
   "outputs": [],
   "source": [
    "### Callbacks\n",
    "callback_es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "callback_mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_folder+'/weights.{epoch:02d}-{val_loss:.2f}-{val_accuracy:.2f}.hdf5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hIOwUeQ0E0LD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "executionInfo": {
     "elapsed": 62179,
     "status": "error",
     "timestamp": 1655027119525,
     "user": {
      "displayName": "QUEK HAO YONG, GABRIEL _",
      "userId": "08861584446371432378"
     },
     "user_tz": -480
    },
    "id": "hIOwUeQ0E0LD",
    "outputId": "f4e3403f-db32-44e4-fa6d-b48809a04098"
   },
   "outputs": [],
   "source": [
    "#history = model.fit(x = train_dataset, validation_data = val_dataset, epochs = epochs, callbacks = [callback_es, callback_mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "-TFMGWpszqVi",
   "metadata": {
    "id": "-TFMGWpszqVi"
   },
   "outputs": [],
   "source": [
    "#model.save_weights(f'{model_folder}/final_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3e556d8",
   "metadata": {
    "id": "b3e556d8"
   },
   "outputs": [],
   "source": [
    "model.load_weights(f'{model_folder}/final_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "wo0aCkdPL2EU",
   "metadata": {
    "id": "wo0aCkdPL2EU"
   },
   "outputs": [],
   "source": [
    "# prompts = ['Whenever I think back', 'And so this I know',\n",
    "#            'I am tired of being what you want me to be', 'Feeling so faithless, lost under the surface',\n",
    "#            'Relight our fire, we will find our way', 'We will rise stronger together']\n",
    "# result_strings = {}\n",
    "# results = {}\n",
    "# for prompt in prompts:\n",
    "#     result_str, result = utils.generate_text(model,\n",
    "#                                              ae_utils.ind_to_input_fun, ae_utils.update_input_fun,\n",
    "#                                              start_string = prompt,\n",
    "#                                              window_length = window_len,\n",
    "#                                              vocab_to_index_dict = vocab_to_index, index_to_vocab_dict = index_to_vocab,\n",
    "#                                              vocab_size = vocab_size,\n",
    "#                                              num_generate = 100, temperature = 1.0,\n",
    "#                                              random_seed = random_seed,\n",
    "#                                              end_token = end_token, start_token = start_token,\n",
    "#                                              pad_token = pad_token, unk_token = unk_token,\n",
    "#                                              newline_token = newline_token,\n",
    "#                                              depth = vocab_size)\n",
    "#     result_strings[prompt] = result_str\n",
    "#     results[prompt] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "Ti-2vAfNWHUI",
   "metadata": {
    "id": "Ti-2vAfNWHUI"
   },
   "outputs": [],
   "source": [
    "# print(result_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "PgGgpFvaxDuL",
   "metadata": {
    "id": "PgGgpFvaxDuL"
   },
   "outputs": [],
   "source": [
    "# for k, v in result_strings.items():\n",
    "#     with open(model_folder+f'/human_{model_name}-{utils.remove_punct(k.lower())}.txt', 'w') as f:\n",
    "#         f.write(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "125b1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../../output/prompt_ref.json', 'r') as f:\n",
    "    eval_prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9700275c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-deb0f3535419>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mresult_strings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval_prompts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     result_str, _ = utils.generate_text(model,\n\u001b[0m\u001b[0;32m      4\u001b[0m                                              \u001b[0mae_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mind_to_input_fun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mae_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_input_fun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                              \u001b[0mstart_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\MITB\\Academic\\CS605 NLP\\Group Project\\NLP-Lyric-Generator\\src\\lib\\utilities.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[1;34m(model, ind_to_input_fun, update_input_fun, start_string, window_length, vocab_to_index_dict, index_to_vocab_dict, vocab_size, num_generate, temperature, random_seed, end_token, start_token, pad_token, unk_token, newline_token, mask_token, discard_repeat, temperature_growth_factor, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mcurr_temperature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_generated\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnum_generate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;31m# Using a categorical distribution to predict the character returned by the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1949\u001b[0m               stacklevel=2)\n\u001b[0;32m   1950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1951\u001b[1;33m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1952\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1953\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1398\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    352\u001b[0m     dataset = tf.data.Dataset.zip((\n\u001b[0;32m    353\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m     ))\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    711\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m     \"\"\"\n\u001b[1;32m--> 713\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, element, name)\u001b[0m\n\u001b[0;32m   4459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4460\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_and_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4461\u001b[1;33m     variant_tensor = gen_dataset_ops.tensor_dataset(\n\u001b[0m\u001b[0;32m   4462\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4463\u001b[0m         \u001b[0moutput_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_flat_tensor_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mtensor_dataset\u001b[1;34m(components, output_shapes, metadata, name)\u001b[0m\n\u001b[0;32m   7448\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7449\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7450\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   7451\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TensorDataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7452\u001b[0m         output_shapes, \"metadata\", metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_strings = {}\n",
    "for prompt, actual in eval_prompts.items():\n",
    "    result_str, _ = utils.generate_text(model,\n",
    "                                             ae_utils.ind_to_input_fun, ae_utils.update_input_fun,\n",
    "                                             start_string = prompt,\n",
    "                                             window_length = window_len,\n",
    "                                             vocab_to_index_dict = vocab_to_index, index_to_vocab_dict = index_to_vocab,\n",
    "                                             vocab_size = vocab_size,\n",
    "                                             num_generate = 100, temperature = 1.0,\n",
    "                                             random_seed = random_seed,\n",
    "                                             end_token = end_token, start_token = start_token,\n",
    "                                             pad_token = pad_token, unk_token = unk_token,\n",
    "                                             newline_token = newline_token,\n",
    "                                             discard_repeat = False,\n",
    "                                             depth = vocab_size)\n",
    "    result_strings[prompt] = result_str.replace(newline_token, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5eb3efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in result_strings.items():\n",
    "    with open(model_folder+f'/br_{model_name}-{utils.remove_punct(k.lower())}.txt', 'w') as f:\n",
    "        f.write(v)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Autoencoder (GRU).ipynb",
   "provenance": [
    {
     "file_id": "16XOqdhC_v8cdBOtPn-ouaBFQ4-Fkqe3E",
     "timestamp": 1655029031586
    },
    {
     "file_id": "1L31juRVcjedsJQLb65vDeIYnGfc5VeMn",
     "timestamp": 1654812818685
    },
    {
     "file_id": "1s0I-h_H-57P4mfHpn7K9ARRffBzA2996",
     "timestamp": 1654811271378
    },
    {
     "file_id": "1fSgHJcraq0bKZQlGXUqlQ4abpWOsgdIu",
     "timestamp": 1654782642697
    },
    {
     "file_id": "1_pyvxTi14GzEPtSGxMTy55XDNlCfsK0h",
     "timestamp": 1654781952290
    },
    {
     "file_id": "164GHOXuG8X-6WN_mbIfShYez3xOdSrkL",
     "timestamp": 1654771075102
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
