{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/SMU_MITB_NLP/Group Project/NLP-Lyric-Generator/src/bin"
      ],
      "metadata": {
        "id": "1sqwH29gCCae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab998de-5dcf-4e76-c11f-73c0e5dbb501"
      },
      "id": "1sqwH29gCCae",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/SMU_MITB_NLP/Group Project/NLP-Lyric-Generator/src/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed94d9c",
      "metadata": {
        "id": "eed94d9c"
      },
      "outputs": [],
      "source": [
        "### Standard Imports\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f77d0b",
      "metadata": {
        "id": "18f77d0b"
      },
      "outputs": [],
      "source": [
        "### Custom Imports\n",
        "sys.path.append('../')\n",
        "import lib.utilities as utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Text Parameters\n",
        "start_token = '<cls>'\n",
        "end_token = '<eos>'\n",
        "pad_token = '<pad>'\n",
        "\n",
        "### General Parameters\n",
        "random_seed = 2022\n",
        "model_folder = '../../models/autoencoder/lstm/v3'\n",
        "\n",
        "### Model Parameters\n",
        "val_split = 0.2\n",
        "window_len = 15\n",
        "batch_size = 64\n",
        "embedding_dim = 128\n",
        "enc_dim, dec_dim = 256, 256\n",
        "learn_rate = 0.001\n",
        "epochs = 80\n",
        "dropout = 0.05\n",
        "recurrent_dropout = 0.05"
      ],
      "metadata": {
        "id": "QsvWeCUNxSvd"
      },
      "id": "QsvWeCUNxSvd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f082fd",
      "metadata": {
        "id": "e1f082fd"
      },
      "outputs": [],
      "source": [
        "### Load Data\n",
        "corpus = utils.load_corpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52adf0e5",
      "metadata": {
        "scrolled": true,
        "id": "52adf0e5"
      },
      "outputs": [],
      "source": [
        "### Pre-Processing Text\n",
        "words = utils.preprocess_text(corpus, fun_list = [utils.to_lower, utils.decontraction, utils.remove_punct], keep = '\\<|\\>')\n",
        "words = re.sub('\\n',' \\n ', words)\n",
        "words = re.split(' +', words) #Tokenising\n",
        "\n",
        "word_count = Counter(words) # Assumes end_token is already in the corpus\n",
        "word_count[start_token] = 0\n",
        "word_count[pad_token] = 0\n",
        "\n",
        "#Reference Dictionaries to convert one-hot index to string and vice versa\n",
        "index_to_vocab = {i: k for i, k in enumerate(word_count.keys())}\n",
        "vocab_to_index = {k: i for i, k in enumerate(word_count.keys())}\n",
        "\n",
        "songs = ' '.join(words)\n",
        "songs = songs.split(' \\n \\n <eos> \\n \\n ')\n",
        "songs = [song.split(' ') for song in songs]\n",
        "songs = [[pad_token]*(window_len-1) + [start_token] + song + [end_token] + [pad_token]*(window_len-1) for song in songs]\n",
        "songs_token_ind = [[vocab_to_index.get(x) for x in song] for song in songs]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating Dataset\n",
        "x_encoder = []\n",
        "x_decoder = []\n",
        "y = []\n",
        "vocab_size = len(word_count)\n",
        "\n",
        "for song in songs_token_ind:\n",
        "  for i in range(len(song)-window_len):\n",
        "    x_encoder.append(song[(i+1):(i+window_len+1)])\n",
        "    x_decoder.append(song[i:(i+window_len)])\n",
        "    y.append(song[i+window_len])"
      ],
      "metadata": {
        "id": "FwuVvlRF-cgL"
      },
      "id": "FwuVvlRF-cgL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_int = np.random.randint(0, len(x_encoder), 1)[0]\n",
        "print([index_to_vocab.get(x) for x in x_encoder[rand_int]])\n",
        "print([index_to_vocab.get(x) for x in x_decoder[rand_int]])\n",
        "print(index_to_vocab.get(y[rand_int]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY-CHZ0CcP2C",
        "outputId": "ffd4ba59-57d9-4149-ceba-f1e510f0fa16"
      },
      "id": "KY-CHZ0CcP2C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['future', '\\n', 'together', 'you', 'and', 'i', 'we', 'are', '\\n', '\\n', '<chorus>', '\\n', 'one', 'singapore', '\\n']\n",
            "['brighter', 'future', '\\n', 'together', 'you', 'and', 'i', 'we', 'are', '\\n', '\\n', '<chorus>', '\\n', 'one', 'singapore']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(((x_encoder, x_decoder), y))\n",
        "dataset = dataset.shuffle(buffer_size = 10000, seed = random_seed)\n",
        "train_dataset = dataset.take(int((1-val_split)*len(dataset)))\n",
        "val_dataset = dataset.skip(int((1-val_split)*len(dataset)))\n",
        "\n",
        "train_dataset_final = train_dataset.batch(batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_dataset_final = val_dataset.batch(batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Dk3I2VI1Nza3"
      },
      "id": "Dk3I2VI1Nza3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "202bf8be",
      "metadata": {
        "id": "202bf8be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff0272f-ca93-48d1-be52-a4294da370db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)     [(None, 15, 1049)]   0           []                               \n",
            "                                                                                                  \n",
            " decoder_input (InputLayer)     [(None, 15, 1049)]   0           []                               \n",
            "                                                                                                  \n",
            " encoder_lstm (LSTM)            [(None, 256),        1337344     ['encoder_input[0][0]']          \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)            (None, 256)          1337344     ['decoder_input[0][0]',          \n",
            "                                                                  'encoder_lstm[0][1]',           \n",
            "                                                                  'encoder_lstm[0][2]']           \n",
            "                                                                                                  \n",
            " attention (Attention)          (None, 256)          0           ['decoder_lstm[0][0]',           \n",
            "                                                                  'encoder_lstm[0][0]']           \n",
            "                                                                                                  \n",
            " tf.concat_1 (TFOpLambda)       (None, 512)          0           ['decoder_lstm[0][0]',           \n",
            "                                                                  'attention[0][0]']              \n",
            "                                                                                                  \n",
            " output (Dense)                 (None, 1049)         538137      ['tf.concat_1[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,212,825\n",
            "Trainable params: 3,212,825\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Encoder\n",
        "encoder_input = layers.Input(shape=(window_len), name = 'encoder_input')\n",
        "encoder_embedded = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length = window_len, name = 'encoder_embedding')(\n",
        "    encoder_input\n",
        ")\n",
        "\n",
        "# Return state in addition to output\n",
        "encoder_output, encoder_hidden_state, encoder_cell_state = layers.LSTM(enc_dim,\n",
        "                                                                       dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
        "                                                                       return_state=True, name = \"encoder_lstm\")(encoder_embedded)\n",
        "\n",
        "# Decoder\n",
        "decoder_input = layers.Input(shape=(window_len), name = 'decoder_input')\n",
        "decoder_embedded = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length = window_len, name = 'decoder_embedding')(\n",
        "    decoder_input\n",
        ")\n",
        "\n",
        "# Pass the encoder state to a new LSTM, as initial state\n",
        "decoder_output = layers.LSTM(dec_dim,\n",
        "                             dropout = dropout, recurrent_dropout = recurrent_dropout,\n",
        "                             name=\"decoder_lstm\")(decoder_embedded, initial_state=[encoder_hidden_state, encoder_cell_state])\n",
        "\n",
        "# Attention\n",
        "attention_context_vector = tf.keras.layers.Attention(name = 'attention')(inputs = [decoder_output, encoder_output])\n",
        "\n",
        "# Output\n",
        "output = layers.Dense(vocab_size, name = 'output', activation = 'softmax')(tf.concat([decoder_output, attention_context_vector], 1))\n",
        "\n",
        "model = tf.keras.Model((encoder_input, decoder_input), output, name = 'LSTM Encoder-Decoder with Dense Embeddings and Attention')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83bb01d4",
      "metadata": {
        "id": "83bb01d4"
      },
      "outputs": [],
      "source": [
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate),\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Callbacks\n",
        "callback_es = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0,\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "callback_mc = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='weights.{epoch:02d}-{val_loss:.2f}-{val_accuracy:.2f}.hdf5',\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)"
      ],
      "metadata": {
        "id": "oxvZ57pCIWWu"
      },
      "id": "oxvZ57pCIWWu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x = train_dataset_final, validation_data = val_dataset_final, epochs = epochs, callbacks = [callback_es, callback_mc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIOwUeQ0E0LD",
        "outputId": "75206bbd-3829-4085-affd-6f23b6822fa4"
      },
      "id": "hIOwUeQ0E0LD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "181/181 [==============================] - 133s 705ms/step - loss: 5.0912 - accuracy: 0.1840 - val_loss: 4.6535 - val_accuracy: 0.2197\n",
            "Epoch 2/80\n",
            "181/181 [==============================] - 124s 684ms/step - loss: 4.2379 - accuracy: 0.2520 - val_loss: 3.5873 - val_accuracy: 0.2997\n",
            "Epoch 3/80\n",
            "181/181 [==============================] - 126s 694ms/step - loss: 3.0037 - accuracy: 0.4063 - val_loss: 2.2561 - val_accuracy: 0.5703\n",
            "Epoch 4/80\n",
            "181/181 [==============================] - 124s 686ms/step - loss: 2.0851 - accuracy: 0.5957 - val_loss: 1.6548 - val_accuracy: 0.6895\n",
            "Epoch 5/80\n",
            "181/181 [==============================] - 126s 697ms/step - loss: 1.6361 - accuracy: 0.6735 - val_loss: 1.3503 - val_accuracy: 0.7266\n",
            "Epoch 6/80\n",
            "181/181 [==============================] - 126s 699ms/step - loss: 1.3772 - accuracy: 0.7214 - val_loss: 1.2353 - val_accuracy: 0.7495\n",
            "Epoch 7/80\n",
            "181/181 [==============================] - 126s 695ms/step - loss: 1.2427 - accuracy: 0.7422 - val_loss: 1.1605 - val_accuracy: 0.7564\n",
            "Epoch 8/80\n",
            "140/181 [======================>.......] - ETA: 26s - loss: 1.1067 - accuracy: 0.7614"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(model_folder)"
      ],
      "metadata": {
        "id": "QhFHlI6o06Gz"
      },
      "id": "QhFHlI6o06Gz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, num_generate = 1000, temperature=1.0, random_seed = random_seed):\n",
        "    # Converting our start string to numbers (vectorizing).\n",
        "    input_indices = [vocab_to_index.get(s) for i, s in enumerate(start_string) if i < window_len-1]\n",
        "    input_indices = [vocab_to_index.get(pad_token)]*(window_len - len(input_indices)-1) + [vocab_to_index.get(start_token)] + input_indices\n",
        "\n",
        "    input_oh = tf.one_hot(input_indices, depth = vocab_size)\n",
        "    x = tf.expand_dims(input_oh, 0)\n",
        "\n",
        "    # Empty string to store our results.\n",
        "    text_generated = []\n",
        "\n",
        "    # Here batch size == 1.\n",
        "    model.reset_states()\n",
        "    for word_index in range(num_generate):\n",
        "        prediction = model.predict([x,x])\n",
        "\n",
        "        # Using a categorical distribution to predict the character returned by the model.\n",
        "        prediction = prediction / temperature\n",
        "        predicted_id = tf.random.categorical(prediction, num_samples=1, seed = random_seed)[-1,0]\n",
        "        predicted_oh = tf.one_hot(predicted_id, depth = vocab_size)\n",
        "\n",
        "        # We pass the series of previous words (up to window length) as the next input to the model\n",
        "        # along with the previous hidden state.\n",
        "        input_index = tf.expand_dims([predicted_oh], 0)\n",
        "        x = tf.concat([x[:,1:,:],input_index], 1)\n",
        "        \n",
        "        pred_word = index_to_vocab[predicted_id.numpy()]\n",
        "        text_generated.append(pred_word)\n",
        "        if pred_word == end_token:\n",
        "          break\n",
        "    \n",
        "    return (' '.join(start_string) + ' ' + ' '.join(text_generated)), text_generated"
      ],
      "metadata": {
        "id": "_ANJJNQGSpdx"
      },
      "id": "_ANJJNQGSpdx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_str, result = generate_text(model, start_string=['<verse>','\\n','step','by','step'], num_generate=50, temperature=1.0)"
      ],
      "metadata": {
        "id": "wo0aCkdPL2EU"
      },
      "id": "wo0aCkdPL2EU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result_str)"
      ],
      "metadata": {
        "id": "smPNycjBnDXa"
      },
      "id": "smPNycjBnDXa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PgGgpFvaxDuL"
      },
      "id": "PgGgpFvaxDuL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Autoencoder (Dense Embedding and LSTM) with Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}