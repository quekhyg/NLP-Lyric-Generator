{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "587e21d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'bring', 'us', 'closer', 'every', 'day', '<new>', '<new>', '<verse>', '<new>', 'this', 'is', 'a', 'place', '<new>']\n",
      "['<new>', 'can', 'bring', 'us', 'closer', 'every', 'day', '<new>', '<new>', '<verse>', '<new>', 'this', 'is', 'a', 'place']\n",
      "<new>\n",
      "['difference', '<new>', 'our', 'friends', 'our', 'families', '<new>', 'we', 'will', 'share', 'a', 'special', 'dream', '<new>', 'together']\n",
      "['a', 'difference', '<new>', 'our', 'friends', 'our', 'families', '<new>', 'we', 'will', 'share', 'a', 'special', 'dream', '<new>']\n",
      "together\n",
      "Model: \"ae_rnn\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder_rnn (SimpleRNN)        [(None, 256),        332544      ['encoder_input[0][0]']          \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_rnn (SimpleRNN)        (None, 256)          332544      ['decoder_input[0][0]',          \n",
      "                                                                  'encoder_rnn[0][1]']            \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1042)         267794      ['decoder_rnn[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 932,882\n",
      "Trainable params: 932,882\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "185/185 [==============================] - 19s 90ms/step - loss: 5.0527 - accuracy: 0.1833 - val_loss: 4.7178 - val_accuracy: 0.2065\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 16s 88ms/step - loss: 3.9400 - accuracy: 0.3090 - val_loss: 3.3317 - val_accuracy: 0.4451\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 2.4387 - accuracy: 0.5812 - val_loss: 1.9248 - val_accuracy: 0.7118\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 15s 83ms/step - loss: 1.5732 - accuracy: 0.7368 - val_loss: 1.4953 - val_accuracy: 0.7946\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 15s 81ms/step - loss: 1.1568 - accuracy: 0.8077 - val_loss: 1.2720 - val_accuracy: 0.8356\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 16s 85ms/step - loss: 0.9045 - accuracy: 0.8487 - val_loss: 1.1899 - val_accuracy: 0.8570\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 16s 85ms/step - loss: 0.7522 - accuracy: 0.8762 - val_loss: 1.0765 - val_accuracy: 0.8800\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.6365 - accuracy: 0.8965 - val_loss: 1.0898 - val_accuracy: 0.8826\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.5811 - accuracy: 0.9057 - val_loss: 1.0407 - val_accuracy: 0.8875\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.5075 - accuracy: 0.9144 - val_loss: 1.0270 - val_accuracy: 0.8950\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.4784 - accuracy: 0.9207 - val_loss: 1.0268 - val_accuracy: 0.9059\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.4342 - accuracy: 0.9269 - val_loss: 1.0338 - val_accuracy: 0.9022\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.3904 - accuracy: 0.9372 - val_loss: 1.0456 - val_accuracy: 0.9052\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.3784 - accuracy: 0.9413 - val_loss: 1.0932 - val_accuracy: 0.9059\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.3713 - accuracy: 0.9388 - val_loss: 1.0511 - val_accuracy: 0.9138\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.3435 - accuracy: 0.9481 - val_loss: 1.1159 - val_accuracy: 0.9138\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 16s 88ms/step - loss: 0.3365 - accuracy: 0.9458 - val_loss: 1.1451 - val_accuracy: 0.9135\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.3090 - accuracy: 0.9485 - val_loss: 1.1223 - val_accuracy: 0.9105\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.3229 - accuracy: 0.9469 - val_loss: 1.1245 - val_accuracy: 0.9108\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.3055 - accuracy: 0.9521 - val_loss: 1.1111 - val_accuracy: 0.9161\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.9497Restoring model weights from the end of the best epoch: 11.\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.3094 - accuracy: 0.9497 - val_loss: 1.0683 - val_accuracy: 0.9187\n",
      "Epoch 21: early stopping\n",
      "{'Whenever I think back': 'Whenever I think back flag flag flag flag flag flag flag faces faces faces faces struggled storms waiting why thousand river river kept unfold highest working working lion lion lion lion lion lion lion lion lion to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to dating dating dating wherever wherever wherever wherever wherever because because because because because because', 'And so this I know': 'And so this I know know from from from from from from from from from from from from from from from from from from from far far far far far far far far far them them them understand prospered hoping cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot wildest started asking quay climb climb climb voice going neon alone light light light light light light light light light light light light light light light light light light light light light light light light light spot warmth', 'I am tired of being what you want me to be': 'I am tired of being what you want me to be be be be be be be be be be be be like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like', 'Feeling so faithless, lost under the surface': 'Feeling so faithless, lost under the surface streets streets streets going door gem man man room room room everywhere everywhere everywhere everywhere everywhere fortunate today today today today today today today book life heartbeat heartbeat heartbeat heartbeat heartbeat heartbeat heartbeat heartbeat heartbeat heartbeat heartbeat heartbeat ways faraway', 'Relight our fire, we will find our way': 'Relight our fire, we will find our way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way', 'We will rise stronger together': 'We will rise stronger together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together hear hear hear hear hear hear hear hear hear hear hear hear hear hear hear hear said said said ask vigilance stage stronger stronger stronger beginning highest matter matter matter matter matter matter matter matter'}\n"
     ]
    }
   ],
   "source": [
    "%run autoencoder_rnn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e22aee4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heart', 'our', 'mind', 'our', 'will', 'forever', 'more', '<new>', '<new>', '<verse>', '<new>', 'what', 'we', 'have', 'is']\n",
      "['our', 'heart', 'our', 'mind', 'our', 'will', 'forever', 'more', '<new>', '<new>', '<verse>', '<new>', 'what', 'we', 'have']\n",
      "is\n",
      "['our', 'defence', '<new>', 'put', 'your', 'heart', 'your', 'mind', 'your', 'skill', 'to', 'our', 'defence', '<new>', 'put']\n",
      "['to', 'our', 'defence', '<new>', 'put', 'your', 'heart', 'your', 'mind', 'your', 'skill', 'to', 'our', 'defence', '<new>']\n",
      "put\n",
      "Model: \"ae_rnn_att\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder_rnn (SimpleRNN)        [(None, 256),        332544      ['encoder_input[0][0]']          \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_rnn (SimpleRNN)        (None, 256)          332544      ['decoder_input[0][0]',          \n",
      "                                                                  'encoder_rnn[0][1]']            \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 256)          0           ['decoder_rnn[0][0]',            \n",
      "                                                                  'encoder_rnn[0][0]']            \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 512)          0           ['decoder_rnn[0][0]',            \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1042)         534546      ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,199,634\n",
      "Trainable params: 1,199,634\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "185/185 [==============================] - 18s 87ms/step - loss: 4.8731 - accuracy: 0.2085 - val_loss: 4.4444 - val_accuracy: 0.2524\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 3.5634 - accuracy: 0.3836 - val_loss: 2.9485 - val_accuracy: 0.5211\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 2.1680 - accuracy: 0.6471 - val_loss: 1.7687 - val_accuracy: 0.7532\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 16s 85ms/step - loss: 1.3902 - accuracy: 0.7752 - val_loss: 1.3965 - val_accuracy: 0.8254\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 15s 84ms/step - loss: 1.0385 - accuracy: 0.8284 - val_loss: 1.2510 - val_accuracy: 0.8503\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 16s 85ms/step - loss: 0.8271 - accuracy: 0.8672 - val_loss: 1.1768 - val_accuracy: 0.8653\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.7076 - accuracy: 0.8865 - val_loss: 1.1462 - val_accuracy: 0.8740\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 15s 83ms/step - loss: 0.5986 - accuracy: 0.9040 - val_loss: 1.0607 - val_accuracy: 0.8901\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 15s 83ms/step - loss: 0.5727 - accuracy: 0.9067 - val_loss: 1.0799 - val_accuracy: 0.8837\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.5474 - accuracy: 0.9083 - val_loss: 1.0546 - val_accuracy: 0.8924\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.5100 - accuracy: 0.9166 - val_loss: 1.0394 - val_accuracy: 0.8943\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.4754 - accuracy: 0.9196 - val_loss: 1.0691 - val_accuracy: 0.8913\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 15s 83ms/step - loss: 0.4284 - accuracy: 0.9282 - val_loss: 1.0482 - val_accuracy: 0.9026\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.4166 - accuracy: 0.9313 - val_loss: 1.0105 - val_accuracy: 0.9037\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.4194 - accuracy: 0.9305 - val_loss: 1.0385 - val_accuracy: 0.9041\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.3691 - accuracy: 0.9401 - val_loss: 0.9872 - val_accuracy: 0.9090\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 16s 85ms/step - loss: 0.3352 - accuracy: 0.9478 - val_loss: 0.9847 - val_accuracy: 0.9146\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.3310 - accuracy: 0.9463 - val_loss: 1.0137 - val_accuracy: 0.9097\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.3530 - accuracy: 0.9434 - val_loss: 0.9877 - val_accuracy: 0.9105\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.3412 - accuracy: 0.9436 - val_loss: 0.9819 - val_accuracy: 0.9108\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 15s 83ms/step - loss: 0.3146 - accuracy: 0.9475 - val_loss: 0.9905 - val_accuracy: 0.9165\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 16s 87ms/step - loss: 0.3366 - accuracy: 0.9449 - val_loss: 1.0005 - val_accuracy: 0.9146\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 15s 84ms/step - loss: 0.3254 - accuracy: 0.9455 - val_loss: 1.0095 - val_accuracy: 0.9138\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 16s 85ms/step - loss: 0.2938 - accuracy: 0.9524 - val_loss: 0.9958 - val_accuracy: 0.9240\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.3065 - accuracy: 0.9513 - val_loss: 1.0062 - val_accuracy: 0.9176\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 16s 85ms/step - loss: 0.2995 - accuracy: 0.9501 - val_loss: 0.9834 - val_accuracy: 0.9221\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.3016 - accuracy: 0.9513 - val_loss: 1.0001 - val_accuracy: 0.9195\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.3336 - accuracy: 0.9439 - val_loss: 0.9976 - val_accuracy: 0.9202\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 16s 84ms/step - loss: 0.3212 - accuracy: 0.9444 - val_loss: 1.0200 - val_accuracy: 0.9191\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.9497Restoring model weights from the end of the best epoch: 20.\n",
      "185/185 [==============================] - 16s 86ms/step - loss: 0.2923 - accuracy: 0.9497 - val_loss: 1.0114 - val_accuracy: 0.9195\n",
      "Epoch 30: early stopping\n",
      "{'Whenever I think back': 'Whenever I think back back tired tired tired tired always always always my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my always always my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my my', 'And so this I know': 'And so this I know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know every every every every every every every will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will will', 'I am tired of being what you want me to be': 'I am tired of being what you want me to be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh oh', 'Feeling so faithless, lost under the surface': 'Feeling so faithless, lost under the surface wonder wonder wonder wonder wonder wonder <prechorus> <prechorus> <prechorus> <prechorus> <prechorus> <prechorus> <prechorus> <prechorus> <prechorus> <prechorus> <prechorus> <prechorus> <prechorus> <prechorus> <prechorus> in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in', 'Relight our fire, we will find our way': 'Relight our fire, we will find our way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way homeland homeland homeland homeland light light light light light light light light light light anything anything anything anything anything anything anything anything smile smile smile smile smile smile smile smile smile smile feel feel feel feel', 'We will rise stronger together': 'We will rise stronger together like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like like built built lost lost lost lost lost lost lost lost lost lost lost lost lost each each each anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything seen seen seen seen fire fabric fabric should brings brings brings brings'}\n"
     ]
    }
   ],
   "source": [
    "%run autoencoder_rnn_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced2082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'be', '<new>', 'the', 'time', 'has', 'come', 'for', 'me', '<new>', 'to', 'strive', 'and', 'to', 'achieve']\n",
      "['it', 'can', 'be', '<new>', 'the', 'time', 'has', 'come', 'for', 'me', '<new>', 'to', 'strive', 'and', 'to']\n",
      "achieve\n",
      "['will', 'stay', 'as', 'one', 'nation', 'undivided', '<new>', 'back', 'to', 'back', 'together', 'we', 'will', 'brave', 'the']\n",
      "['we', 'will', 'stay', 'as', 'one', 'nation', 'undivided', '<new>', 'back', 'to', 'back', 'together', 'we', 'will', 'brave']\n",
      "the\n",
      "WARNING:tensorflow:Layer encoder_gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer decoder_gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"ae_gru\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder_gru (GRU)              [(None, 256),        998400      ['encoder_input[0][0]']          \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_gru (GRU)              (None, 256)          998400      ['decoder_input[0][0]',          \n",
      "                                                                  'encoder_gru[0][1]']            \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1042)         267794      ['decoder_gru[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,264,594\n",
      "Trainable params: 2,264,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "185/185 [==============================] - 54s 272ms/step - loss: 4.7951 - accuracy: 0.1889 - val_loss: 4.2224 - val_accuracy: 0.2077\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 49s 265ms/step - loss: 3.7565 - accuracy: 0.2639 - val_loss: 3.6535 - val_accuracy: 0.3055\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 50s 270ms/step - loss: 2.9975 - accuracy: 0.3918 - val_loss: 3.0981 - val_accuracy: 0.4229\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 53s 285ms/step - loss: 2.3318 - accuracy: 0.5258 - val_loss: 2.3178 - val_accuracy: 0.6076\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 50s 269ms/step - loss: 1.8530 - accuracy: 0.6338 - val_loss: 2.1525 - val_accuracy: 0.6859\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 50s 268ms/step - loss: 1.5296 - accuracy: 0.6937 - val_loss: 1.7625 - val_accuracy: 0.7254\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 49s 264ms/step - loss: 1.3027 - accuracy: 0.7359 - val_loss: 1.4734 - val_accuracy: 0.7709\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 51s 278ms/step - loss: 1.1294 - accuracy: 0.7635 - val_loss: 1.3546 - val_accuracy: 0.7792\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 49s 266ms/step - loss: 0.9692 - accuracy: 0.7938 - val_loss: 1.1852 - val_accuracy: 0.8111\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 50s 268ms/step - loss: 0.8343 - accuracy: 0.8268 - val_loss: 1.1014 - val_accuracy: 0.8322\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 50s 268ms/step - loss: 0.7391 - accuracy: 0.8436 - val_loss: 1.0900 - val_accuracy: 0.8307\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 51s 273ms/step - loss: 0.6551 - accuracy: 0.8601 - val_loss: 1.0081 - val_accuracy: 0.8544\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 50s 270ms/step - loss: 0.6056 - accuracy: 0.8730 - val_loss: 0.9887 - val_accuracy: 0.8631\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 50s 268ms/step - loss: 0.5288 - accuracy: 0.8890 - val_loss: 1.0150 - val_accuracy: 0.8649\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 52s 281ms/step - loss: 0.4832 - accuracy: 0.9022 - val_loss: 0.9275 - val_accuracy: 0.8627\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 51s 278ms/step - loss: 0.4438 - accuracy: 0.9085 - val_loss: 1.2423 - val_accuracy: 0.8713\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 49s 268ms/step - loss: 0.3905 - accuracy: 0.9202 - val_loss: 0.8999 - val_accuracy: 0.8890\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 50s 269ms/step - loss: 0.3641 - accuracy: 0.9279 - val_loss: 0.9031 - val_accuracy: 0.8924\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 49s 267ms/step - loss: 0.3486 - accuracy: 0.9335 - val_loss: 0.8491 - val_accuracy: 0.8962\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 50s 272ms/step - loss: 0.2801 - accuracy: 0.9488 - val_loss: 0.9379 - val_accuracy: 0.8950\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 49s 266ms/step - loss: 0.2490 - accuracy: 0.9545 - val_loss: 0.7741 - val_accuracy: 0.9120\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 50s 269ms/step - loss: 0.2166 - accuracy: 0.9626 - val_loss: 0.8282 - val_accuracy: 0.9052\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 50s 269ms/step - loss: 0.2312 - accuracy: 0.9599 - val_loss: 0.8866 - val_accuracy: 0.8822\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 50s 270ms/step - loss: 0.1925 - accuracy: 0.9691 - val_loss: 0.8660 - val_accuracy: 0.9131\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 49s 267ms/step - loss: 0.1720 - accuracy: 0.9725 - val_loss: 0.8568 - val_accuracy: 0.8965\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 49s 267ms/step - loss: 0.1699 - accuracy: 0.9719 - val_loss: 0.8020 - val_accuracy: 0.9195\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 49s 268ms/step - loss: 0.1355 - accuracy: 0.9791 - val_loss: 0.8759 - val_accuracy: 0.9131\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 49s 265ms/step - loss: 0.1369 - accuracy: 0.9775 - val_loss: 0.7736 - val_accuracy: 0.9161\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 50s 268ms/step - loss: 0.1186 - accuracy: 0.9809 - val_loss: 0.7456 - val_accuracy: 0.9195\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 52s 280ms/step - loss: 0.1357 - accuracy: 0.9764 - val_loss: 0.8523 - val_accuracy: 0.9187\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 50s 268ms/step - loss: 0.0932 - accuracy: 0.9860 - val_loss: 0.7380 - val_accuracy: 0.9217\n",
      "Epoch 32/50\n",
      "185/185 [==============================] - 52s 282ms/step - loss: 0.0831 - accuracy: 0.9860 - val_loss: 0.8819 - val_accuracy: 0.9214\n",
      "Epoch 33/50\n",
      "185/185 [==============================] - 50s 269ms/step - loss: 0.0969 - accuracy: 0.9812 - val_loss: 0.7960 - val_accuracy: 0.9248\n",
      "Epoch 34/50\n",
      "185/185 [==============================] - 50s 268ms/step - loss: 0.0782 - accuracy: 0.9852 - val_loss: 0.9159 - val_accuracy: 0.9225\n",
      "Epoch 35/50\n",
      "185/185 [==============================] - 49s 266ms/step - loss: 0.0608 - accuracy: 0.9890 - val_loss: 0.8927 - val_accuracy: 0.9229\n",
      "Epoch 36/50\n",
      "185/185 [==============================] - 50s 269ms/step - loss: 0.0589 - accuracy: 0.9887 - val_loss: 0.7274 - val_accuracy: 0.9300\n",
      "Epoch 37/50\n",
      "185/185 [==============================] - 50s 272ms/step - loss: 0.0619 - accuracy: 0.9884 - val_loss: 0.7423 - val_accuracy: 0.9172\n",
      "Epoch 38/50\n",
      "185/185 [==============================] - 50s 271ms/step - loss: 0.0641 - accuracy: 0.9876 - val_loss: 0.9102 - val_accuracy: 0.9105\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 50s 270ms/step - loss: 0.0629 - accuracy: 0.9880 - val_loss: 0.7648 - val_accuracy: 0.9127\n",
      "Epoch 40/50\n",
      "185/185 [==============================] - 50s 268ms/step - loss: 0.0863 - accuracy: 0.9819 - val_loss: 0.8119 - val_accuracy: 0.9210\n",
      "Epoch 41/50\n",
      "185/185 [==============================] - 49s 266ms/step - loss: 0.0519 - accuracy: 0.9907 - val_loss: 0.8256 - val_accuracy: 0.9217\n",
      "Epoch 42/50\n",
      "185/185 [==============================] - 51s 278ms/step - loss: 0.0594 - accuracy: 0.9888 - val_loss: 0.8443 - val_accuracy: 0.9221\n",
      "Epoch 43/50\n",
      "185/185 [==============================] - 52s 279ms/step - loss: 0.0490 - accuracy: 0.9903 - val_loss: 0.9773 - val_accuracy: 0.9206\n",
      "Epoch 44/50\n",
      "185/185 [==============================] - 50s 272ms/step - loss: 0.0562 - accuracy: 0.9883 - val_loss: 0.7358 - val_accuracy: 0.9176\n",
      "Epoch 45/50\n",
      "185/185 [==============================] - 49s 267ms/step - loss: 0.0727 - accuracy: 0.9847 - val_loss: 0.7742 - val_accuracy: 0.9210\n",
      "Epoch 46/50\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9906Restoring model weights from the end of the best epoch: 36.\n",
      "185/185 [==============================] - 50s 271ms/step - loss: 0.0522 - accuracy: 0.9906 - val_loss: 0.9770 - val_accuracy: 0.9120\n",
      "Epoch 46: early stopping\n",
      "{'Whenever I think back': 'Whenever I think back back back back back back back back back back back back determination determination determination back back back back away away away things things things things things things things things things things things things things things things things things things things things things things things things things things things things things things whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa whoa', 'And so this I know': 'And so this I know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know every every every every every every every every every every every every every every every every every every every every every every every every every every every every every every every every every every every every every every every look look look look look look look look look look look look', 'I am tired of being what you want me to be': 'I am tired of being what you want me to be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be', 'Feeling so faithless, lost under the surface': 'Feeling so faithless, lost under the surface chains hard hard tiny binds highest song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song song', 'Relight our fire, we will find our way': 'Relight our fire, we will find our way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way', 'We will rise stronger together': 'We will rise stronger together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together'}\n"
     ]
    }
   ],
   "source": [
    "%run autoencoder_gru.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8932ee3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'roamed', 'the', 'world', '<new>', 'it', 'is', 'still', 'my', 'home', 'i', 'long', 'to', 'see']\n",
      "['though', 'i', 'have', 'roamed', 'the', 'world', '<new>', 'it', 'is', 'still', 'my', 'home', 'i', 'long', 'to']\n",
      "see\n",
      "['happen', '<new>', 'will', 'you', 'let', 'your', 'dreams', 'take', 'flight', '<new>', 'and', 'will', 'you', 'make', 'the']\n",
      "['it', 'happen', '<new>', 'will', 'you', 'let', 'your', 'dreams', 'take', 'flight', '<new>', 'and', 'will', 'you', 'make']\n",
      "the\n",
      "WARNING:tensorflow:Layer encoder_gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer decoder_gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"ae_gru_att\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder_gru (GRU)              [(None, 256),        998400      ['encoder_input[0][0]']          \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_gru (GRU)              (None, 256)          998400      ['decoder_input[0][0]',          \n",
      "                                                                  'encoder_gru[0][1]']            \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 256)          0           ['decoder_gru[0][0]',            \n",
      "                                                                  'encoder_gru[0][0]']            \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, 512)          0           ['decoder_gru[0][0]',            \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1042)         534546      ['tf.concat_1[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,531,346\n",
      "Trainable params: 2,531,346\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "185/185 [==============================] - 54s 272ms/step - loss: 4.9602 - accuracy: 0.2049 - val_loss: 4.6182 - val_accuracy: 0.2129\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 50s 270ms/step - loss: 3.6403 - accuracy: 0.3264 - val_loss: 3.4367 - val_accuracy: 0.4011\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 49s 267ms/step - loss: 2.6385 - accuracy: 0.4917 - val_loss: 2.4680 - val_accuracy: 0.5707\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 52s 279ms/step - loss: 2.0004 - accuracy: 0.6082 - val_loss: 2.2254 - val_accuracy: 0.6144\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 51s 274ms/step - loss: 1.5544 - accuracy: 0.6999 - val_loss: 1.7306 - val_accuracy: 0.7468\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 51s 274ms/step - loss: 1.2397 - accuracy: 0.7586 - val_loss: 1.6868 - val_accuracy: 0.7675\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 52s 280ms/step - loss: 0.9905 - accuracy: 0.8090 - val_loss: 1.6068 - val_accuracy: 0.7641\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 50s 272ms/step - loss: 0.8123 - accuracy: 0.8433 - val_loss: 1.5148 - val_accuracy: 0.8025\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 50s 268ms/step - loss: 0.6975 - accuracy: 0.8649 - val_loss: 1.2898 - val_accuracy: 0.8224\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 50s 273ms/step - loss: 0.6095 - accuracy: 0.8798 - val_loss: 1.2139 - val_accuracy: 0.8382\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 51s 275ms/step - loss: 0.5300 - accuracy: 0.8940 - val_loss: 1.5269 - val_accuracy: 0.8164\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 50s 273ms/step - loss: 0.4250 - accuracy: 0.9190 - val_loss: 1.3364 - val_accuracy: 0.8363\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 51s 273ms/step - loss: 0.3451 - accuracy: 0.9394 - val_loss: 1.4524 - val_accuracy: 0.8386\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 51s 276ms/step - loss: 0.3220 - accuracy: 0.9433 - val_loss: 1.4037 - val_accuracy: 0.8363\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 50s 270ms/step - loss: 0.2657 - accuracy: 0.9529 - val_loss: 1.3962 - val_accuracy: 0.8337\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 50s 273ms/step - loss: 0.2243 - accuracy: 0.9604 - val_loss: 1.4595 - val_accuracy: 0.8348\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 50s 271ms/step - loss: 0.1893 - accuracy: 0.9678 - val_loss: 1.3464 - val_accuracy: 0.8439\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 54s 291ms/step - loss: 0.1745 - accuracy: 0.9708 - val_loss: 1.2809 - val_accuracy: 0.8585\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 51s 275ms/step - loss: 0.1508 - accuracy: 0.9733 - val_loss: 1.4004 - val_accuracy: 0.8533\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9745Restoring model weights from the end of the best epoch: 10.\n",
      "185/185 [==============================] - 50s 272ms/step - loss: 0.1387 - accuracy: 0.9745 - val_loss: 1.3822 - val_accuracy: 0.8612\n",
      "Epoch 20: early stopping\n",
      "{'Whenever I think back': 'Whenever I think back troubles touch yea sunshine own just just just just just just keep got alone alone today alone alone alone today everyone everyone everyone everyone everyone everyone today find forget alone alone today alone alone today today alone alone alone sit just just share share in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in', 'And so this I know': 'And so this I know life life life life life life life life life life life life me me me me me me me me me me me me me me see see see see out out nation nation nation nation nation nation with with with harmony \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n', 'I am tired of being what you want me to be': 'I am tired of being what you want me to be be you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you', 'Feeling so faithless, lost under the surface': 'Feeling so faithless, lost under the surface \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n', 'Relight our fire, we will find our way': 'Relight our fire, we will find our way way way way way way way way way way way way light light hearts things join hearts together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together', 'We will rise stronger together': 'We will rise stronger together together together together together together together sing sing sing sing sing sing sing sing sing sing sing sing sing sing sing sing sing sing sing sing sing as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as people one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one one'}\n"
     ]
    }
   ],
   "source": [
    "%run autoencoder_gru_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4b1b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'story', '<new>', 'uneventfullest', 'story', '<new>', 'that', 'ages', 'with', 'each', 'year', 'and', 'birthday', 'cake', '<new>']\n",
      "['my', 'life', 'story', '<new>', 'uneventfullest', 'story', '<new>', 'that', 'ages', 'with', 'each', 'year', 'and', 'birthday', 'cake']\n",
      "<new>\n",
      "['<new>', 'it', 'is', 'a', 'feeling', 'we', 'all', 'share', '<new>', 'we', 'are', 'going', 'to', 'build', 'a']\n",
      "['air', '<new>', 'it', 'is', 'a', 'feeling', 'we', 'all', 'share', '<new>', 'we', 'are', 'going', 'to', 'build']\n",
      "a\n",
      "WARNING:tensorflow:Layer encoder_lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer decoder_lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"ae_lstm\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 256),        1330176     ['encoder_input[0][0]']          \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            (None, 256)          1330176     ['decoder_input[0][0]',          \n",
      "                                                                  'encoder_lstm[0][1]',           \n",
      "                                                                  'encoder_lstm[0][2]']           \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1042)         267794      ['decoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,928,146\n",
      "Trainable params: 2,928,146\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "185/185 [==============================] - 60s 303ms/step - loss: 5.1431 - accuracy: 0.1649 - val_loss: 5.1070 - val_accuracy: 0.1543\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 56s 302ms/step - loss: 4.7825 - accuracy: 0.1954 - val_loss: 4.9261 - val_accuracy: 0.2065\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 56s 303ms/step - loss: 4.5806 - accuracy: 0.2224 - val_loss: 4.8517 - val_accuracy: 0.2088\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 57s 308ms/step - loss: 4.3886 - accuracy: 0.2352 - val_loss: 4.7422 - val_accuracy: 0.2257\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 55s 299ms/step - loss: 4.1627 - accuracy: 0.2628 - val_loss: 4.6662 - val_accuracy: 0.2528\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 56s 303ms/step - loss: 3.9264 - accuracy: 0.2897 - val_loss: 4.6201 - val_accuracy: 0.2675\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 56s 305ms/step - loss: 3.6195 - accuracy: 0.3298 - val_loss: 4.2713 - val_accuracy: 0.3194\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 57s 307ms/step - loss: 3.1390 - accuracy: 0.4017 - val_loss: 3.8090 - val_accuracy: 0.3770\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 58s 316ms/step - loss: 2.5746 - accuracy: 0.4958 - val_loss: 3.3205 - val_accuracy: 0.4725\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 57s 310ms/step - loss: 2.0855 - accuracy: 0.5876 - val_loss: 2.7394 - val_accuracy: 0.5801\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 56s 305ms/step - loss: 1.6172 - accuracy: 0.6792 - val_loss: 2.4898 - val_accuracy: 0.6437\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 57s 307ms/step - loss: 1.2972 - accuracy: 0.7438 - val_loss: 2.3011 - val_accuracy: 0.7088\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 57s 310ms/step - loss: 1.0553 - accuracy: 0.7930 - val_loss: 2.0872 - val_accuracy: 0.7227\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 58s 314ms/step - loss: 0.8714 - accuracy: 0.8325 - val_loss: 2.0805 - val_accuracy: 0.7385\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 57s 308ms/step - loss: 0.7440 - accuracy: 0.8554 - val_loss: 2.0340 - val_accuracy: 0.7562\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 57s 308ms/step - loss: 0.5772 - accuracy: 0.8907 - val_loss: 2.0302 - val_accuracy: 0.7780\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 58s 311ms/step - loss: 0.4690 - accuracy: 0.9130 - val_loss: 1.9024 - val_accuracy: 0.7938\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 58s 311ms/step - loss: 0.3861 - accuracy: 0.9319 - val_loss: 1.8877 - val_accuracy: 0.7980\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 57s 308ms/step - loss: 0.3286 - accuracy: 0.9449 - val_loss: 1.9159 - val_accuracy: 0.8021\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 57s 306ms/step - loss: 0.2648 - accuracy: 0.9591 - val_loss: 2.0222 - val_accuracy: 0.8002\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 57s 307ms/step - loss: 0.2092 - accuracy: 0.9707 - val_loss: 1.8397 - val_accuracy: 0.8100\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 56s 305ms/step - loss: 0.1625 - accuracy: 0.9827 - val_loss: 1.8232 - val_accuracy: 0.8209\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 58s 316ms/step - loss: 0.1378 - accuracy: 0.9857 - val_loss: 1.8231 - val_accuracy: 0.8318\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 58s 313ms/step - loss: 0.0994 - accuracy: 0.9931 - val_loss: 1.9480 - val_accuracy: 0.8202\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 57s 309ms/step - loss: 0.0869 - accuracy: 0.9936 - val_loss: 1.8632 - val_accuracy: 0.8367\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 59s 316ms/step - loss: 0.0733 - accuracy: 0.9944 - val_loss: 1.8543 - val_accuracy: 0.8322\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 56s 305ms/step - loss: 0.0706 - accuracy: 0.9934 - val_loss: 1.9507 - val_accuracy: 0.8164\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 56s 303ms/step - loss: 0.0597 - accuracy: 0.9947 - val_loss: 1.7959 - val_accuracy: 0.8397\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 56s 303ms/step - loss: 0.0490 - accuracy: 0.9963 - val_loss: 1.9159 - val_accuracy: 0.8164\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 56s 300ms/step - loss: 0.0426 - accuracy: 0.9964 - val_loss: 1.8227 - val_accuracy: 0.8382\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 57s 308ms/step - loss: 0.0338 - accuracy: 0.9970 - val_loss: 1.7743 - val_accuracy: 0.8435\n",
      "Epoch 32/50\n",
      "185/185 [==============================] - 56s 304ms/step - loss: 0.0410 - accuracy: 0.9954 - val_loss: 1.7694 - val_accuracy: 0.8427\n",
      "Epoch 33/50\n",
      "185/185 [==============================] - 56s 304ms/step - loss: 0.0255 - accuracy: 0.9983 - val_loss: 1.7097 - val_accuracy: 0.8563\n",
      "Epoch 34/50\n",
      "185/185 [==============================] - 57s 309ms/step - loss: 0.0278 - accuracy: 0.9976 - val_loss: 1.6972 - val_accuracy: 0.8525\n",
      "Epoch 35/50\n",
      "185/185 [==============================] - 57s 308ms/step - loss: 0.0217 - accuracy: 0.9979 - val_loss: 1.8660 - val_accuracy: 0.8484\n",
      "Epoch 36/50\n",
      "185/185 [==============================] - 56s 300ms/step - loss: 0.0234 - accuracy: 0.9975 - val_loss: 1.7908 - val_accuracy: 0.8424\n",
      "Epoch 37/50\n",
      "185/185 [==============================] - 56s 303ms/step - loss: 0.0243 - accuracy: 0.9967 - val_loss: 1.7730 - val_accuracy: 0.8521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "185/185 [==============================] - 56s 303ms/step - loss: 0.0167 - accuracy: 0.9983 - val_loss: 1.7125 - val_accuracy: 0.8514\n",
      "Epoch 39/50\n",
      "185/185 [==============================] - 56s 302ms/step - loss: 0.0164 - accuracy: 0.9985 - val_loss: 1.7356 - val_accuracy: 0.8589\n",
      "Epoch 40/50\n",
      "185/185 [==============================] - 56s 301ms/step - loss: 0.0226 - accuracy: 0.9965 - val_loss: 1.7678 - val_accuracy: 0.8405\n",
      "Epoch 41/50\n",
      "185/185 [==============================] - 56s 302ms/step - loss: 0.0206 - accuracy: 0.9974 - val_loss: 1.8978 - val_accuracy: 0.8262\n",
      "Epoch 42/50\n",
      "185/185 [==============================] - 57s 306ms/step - loss: 0.0289 - accuracy: 0.9949 - val_loss: 1.7455 - val_accuracy: 0.8506\n",
      "Epoch 43/50\n",
      "185/185 [==============================] - 56s 304ms/step - loss: 0.0210 - accuracy: 0.9964 - val_loss: 1.7416 - val_accuracy: 0.8623\n",
      "Epoch 44/50\n",
      "185/185 [==============================] - 56s 303ms/step - loss: 0.0149 - accuracy: 0.9983 - val_loss: 1.6600 - val_accuracy: 0.8646\n",
      "Epoch 45/50\n",
      "185/185 [==============================] - 56s 304ms/step - loss: 0.0186 - accuracy: 0.9976 - val_loss: 1.6633 - val_accuracy: 0.8521\n",
      "Epoch 46/50\n",
      "185/185 [==============================] - 56s 303ms/step - loss: 0.0212 - accuracy: 0.9968 - val_loss: 1.7127 - val_accuracy: 0.8552\n",
      "Epoch 47/50\n",
      "185/185 [==============================] - 56s 305ms/step - loss: 0.0104 - accuracy: 0.9986 - val_loss: 1.7267 - val_accuracy: 0.8567\n",
      "Epoch 48/50\n",
      "185/185 [==============================] - 58s 314ms/step - loss: 0.0125 - accuracy: 0.9980 - val_loss: 1.7223 - val_accuracy: 0.8559\n",
      "Epoch 49/50\n",
      "185/185 [==============================] - 65s 351ms/step - loss: 0.0121 - accuracy: 0.9975 - val_loss: 1.5950 - val_accuracy: 0.8552\n",
      "Epoch 50/50\n",
      "185/185 [==============================] - 58s 312ms/step - loss: 0.0103 - accuracy: 0.9989 - val_loss: 1.5702 - val_accuracy: 0.8623\n",
      "{'Whenever I think back': 'Whenever I think back days from from from from from from from from from from from from from from from from from from gave renewed highest something something something something something something something something when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when', 'And so this I know': 'And so this I know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know know is is is is is is faces faces faces tomorrow tomorrow tomorrow tomorrow tomorrow tomorrow tomorrow is is is is where where where where where where where where where where where where where where where where where where where where where where where where where where where where where where where where where where', 'I am tired of being what you want me to be': 'I am tired of being what you want me to be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be', 'Feeling so faithless, lost under the surface': 'Feeling so faithless, lost under the surface \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n', 'Relight our fire, we will find our way': 'Relight our fire, we will find our way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way way let let let let let let let let let let let is is is is is is where where where where where where where where where where where where where where where where where where where where where where where where where', 'We will rise stronger together': 'We will rise stronger together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together together'}\n"
     ]
    }
   ],
   "source": [
    "%run autoencoder_lstm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53236aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['day', '<new>', 'people', 'come', 'from', 'every', 'nation', '<new>', 'say', 'our', 'food', 'is', 'such', 'a', 'sensation']\n",
      "['every', 'day', '<new>', 'people', 'come', 'from', 'every', 'nation', '<new>', 'say', 'our', 'food', 'is', 'such', 'a']\n",
      "sensation\n",
      "['stay', '<new>', 'one', 'in', 'every', 'way', '<new>', '<new>', '<chorus>', '<new>', 'together', 'we', 'make', 'a', 'difference']\n",
      "['always', 'stay', '<new>', 'one', 'in', 'every', 'way', '<new>', '<new>', '<chorus>', '<new>', 'together', 'we', 'make', 'a']\n",
      "difference\n",
      "WARNING:tensorflow:Layer encoder_lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer decoder_lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"ae_lstm_att\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, 15, 1042)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 256),        1330176     ['encoder_input[0][0]']          \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            (None, 256)          1330176     ['decoder_input[0][0]',          \n",
      "                                                                  'encoder_lstm[0][1]',           \n",
      "                                                                  'encoder_lstm[0][2]']           \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 256)          0           ['decoder_lstm[0][0]',           \n",
      "                                                                  'encoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      " tf.concat_2 (TFOpLambda)       (None, 512)          0           ['decoder_lstm[0][0]',           \n",
      "                                                                  'attention[0][0]']              \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1042)         534546      ['tf.concat_2[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,194,898\n",
      "Trainable params: 3,194,898\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "185/185 [==============================] - 62s 317ms/step - loss: 5.0759 - accuracy: 0.1796 - val_loss: 4.9238 - val_accuracy: 0.2054\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 59s 318ms/step - loss: 4.2898 - accuracy: 0.2444 - val_loss: 3.9663 - val_accuracy: 0.2713\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 59s 321ms/step - loss: 3.0634 - accuracy: 0.3778 - val_loss: 2.7551 - val_accuracy: 0.4526\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 74s 399ms/step - loss: 2.3340 - accuracy: 0.5108 - val_loss: 2.4790 - val_accuracy: 0.5557\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 77s 414ms/step - loss: 1.8365 - accuracy: 0.6222 - val_loss: 2.1077 - val_accuracy: 0.6840\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 79s 427ms/step - loss: 1.4909 - accuracy: 0.6993 - val_loss: 1.9757 - val_accuracy: 0.7235\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 81s 437ms/step - loss: 1.3054 - accuracy: 0.7337 - val_loss: 1.5804 - val_accuracy: 0.7720\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 62s 337ms/step - loss: 1.1355 - accuracy: 0.7630 - val_loss: 1.5895 - val_accuracy: 0.8036\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 63s 339ms/step - loss: 0.9934 - accuracy: 0.7890 - val_loss: 1.4875 - val_accuracy: 0.7938\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 62s 334ms/step - loss: 0.9102 - accuracy: 0.8047 - val_loss: 1.4653 - val_accuracy: 0.8224\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 60s 323ms/step - loss: 0.8036 - accuracy: 0.8283 - val_loss: 1.3507 - val_accuracy: 0.8292\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 60s 323ms/step - loss: 0.7483 - accuracy: 0.8414 - val_loss: 1.3003 - val_accuracy: 0.8495\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 60s 325ms/step - loss: 0.6704 - accuracy: 0.8566 - val_loss: 1.3073 - val_accuracy: 0.8503\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 60s 323ms/step - loss: 0.6383 - accuracy: 0.8617 - val_loss: 1.2478 - val_accuracy: 0.8555\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 60s 323ms/step - loss: 0.6155 - accuracy: 0.8682 - val_loss: 1.2739 - val_accuracy: 0.8495\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 59s 321ms/step - loss: 0.5493 - accuracy: 0.8818 - val_loss: 1.1662 - val_accuracy: 0.8792\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 60s 322ms/step - loss: 0.4993 - accuracy: 0.8927 - val_loss: 1.0796 - val_accuracy: 0.8822\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 60s 322ms/step - loss: 0.4658 - accuracy: 0.8966 - val_loss: 1.2467 - val_accuracy: 0.8702\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 60s 325ms/step - loss: 0.4747 - accuracy: 0.8918 - val_loss: 1.1931 - val_accuracy: 0.8668\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 61s 330ms/step - loss: 0.4679 - accuracy: 0.8949 - val_loss: 1.0983 - val_accuracy: 0.8593\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 59s 320ms/step - loss: 0.4302 - accuracy: 0.9041 - val_loss: 1.0304 - val_accuracy: 0.8774\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 60s 323ms/step - loss: 0.3896 - accuracy: 0.9128 - val_loss: 1.0341 - val_accuracy: 0.8826\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 59s 318ms/step - loss: 0.3762 - accuracy: 0.9160 - val_loss: 1.0700 - val_accuracy: 0.8785\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 58s 312ms/step - loss: 0.3442 - accuracy: 0.9221 - val_loss: 1.0156 - val_accuracy: 0.8856\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 60s 326ms/step - loss: 0.3802 - accuracy: 0.9144 - val_loss: 1.1155 - val_accuracy: 0.8762\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 61s 331ms/step - loss: 0.3071 - accuracy: 0.9305 - val_loss: 1.0868 - val_accuracy: 0.8864\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 60s 325ms/step - loss: 0.3047 - accuracy: 0.9302 - val_loss: 1.1686 - val_accuracy: 0.8928\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 59s 317ms/step - loss: 0.2916 - accuracy: 0.9324 - val_loss: 1.1239 - val_accuracy: 0.8819\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 64s 345ms/step - loss: 0.2714 - accuracy: 0.9400 - val_loss: 1.0784 - val_accuracy: 0.8905\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 73s 395ms/step - loss: 0.2556 - accuracy: 0.9424 - val_loss: 1.0172 - val_accuracy: 0.8947\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 62s 338ms/step - loss: 0.2674 - accuracy: 0.9382 - val_loss: 1.1242 - val_accuracy: 0.8781\n",
      "Epoch 32/50\n",
      " 35/185 [====>.........................] - ETA: 48s - loss: 0.2890 - accuracy: 0.9308"
     ]
    }
   ],
   "source": [
    "%run autoencoder_lstm_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab967a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lstm v3 (with masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a5cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run autoencoder_lstm_with_attention_and_masking.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c02a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run seq2seq_lstm_with_attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dbfad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
